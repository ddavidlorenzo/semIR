%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}


% Jupyter Notebook code cell colors
\definecolor{nbsphinxin}{HTML}{307FC1}
\definecolor{nbsphinxout}{HTML}{BF5B3D}
\definecolor{nbsphinx-code-bg}{HTML}{F5F5F5}
\definecolor{nbsphinx-code-border}{HTML}{E0E0E0}
\definecolor{nbsphinx-stderr}{HTML}{FFDDDD}
% ANSI colors for output streams and traceback highlighting
\definecolor{ansi-black}{HTML}{3E424D}
\definecolor{ansi-black-intense}{HTML}{282C36}
\definecolor{ansi-red}{HTML}{E75C58}
\definecolor{ansi-red-intense}{HTML}{B22B31}
\definecolor{ansi-green}{HTML}{00A250}
\definecolor{ansi-green-intense}{HTML}{007427}
\definecolor{ansi-yellow}{HTML}{DDB62B}
\definecolor{ansi-yellow-intense}{HTML}{B27D12}
\definecolor{ansi-blue}{HTML}{208FFB}
\definecolor{ansi-blue-intense}{HTML}{0065CA}
\definecolor{ansi-magenta}{HTML}{D160C4}
\definecolor{ansi-magenta-intense}{HTML}{A03196}
\definecolor{ansi-cyan}{HTML}{60C6C8}
\definecolor{ansi-cyan-intense}{HTML}{258F8F}
\definecolor{ansi-white}{HTML}{C5C1B4}
\definecolor{ansi-white-intense}{HTML}{A1A6B2}
\definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
\definecolor{ansi-default-inverse-bg}{HTML}{000000}

% Define an environment for non-plain-text code cell outputs (e.g. images)
\makeatletter
\newenvironment{nbsphinxfancyoutput}{%
    % Avoid fatal error with framed.sty if graphics too long to fit on one page
    \let\sphinxincludegraphics\nbsphinxincludegraphics
    \nbsphinx@image@maxheight\textheight
    \advance\nbsphinx@image@maxheight -2\fboxsep   % default \fboxsep 3pt
    \advance\nbsphinx@image@maxheight -2\fboxrule  % default \fboxrule 0.4pt
    \advance\nbsphinx@image@maxheight -\baselineskip
\def\nbsphinxfcolorbox{\spx@fcolorbox{nbsphinx-code-border}{white}}%
\def\FrameCommand{\nbsphinxfcolorbox\nbsphinxfancyaddprompt\@empty}%
\def\FirstFrameCommand{\nbsphinxfcolorbox\nbsphinxfancyaddprompt\sphinxVerbatim@Continues}%
\def\MidFrameCommand{\nbsphinxfcolorbox\sphinxVerbatim@Continued\sphinxVerbatim@Continues}%
\def\LastFrameCommand{\nbsphinxfcolorbox\sphinxVerbatim@Continued\@empty}%
\MakeFramed{\advance\hsize-\width\@totalleftmargin\z@\linewidth\hsize\@setminipage}%
\lineskip=1ex\lineskiplimit=1ex\raggedright%
}{\par\unskip\@minipagefalse\endMakeFramed}
\makeatother
\newbox\nbsphinxpromptbox
\def\nbsphinxfancyaddprompt{\ifvoid\nbsphinxpromptbox\else
    \kern\fboxrule\kern\fboxsep
    \copy\nbsphinxpromptbox
    \kern-\ht\nbsphinxpromptbox\kern-\dp\nbsphinxpromptbox
    \kern-\fboxsep\kern-\fboxrule\nointerlineskip
    \fi}
\newlength\nbsphinxcodecellspacing
\setlength{\nbsphinxcodecellspacing}{0pt}

% Define support macros for attaching opening and closing lines to notebooks
\newsavebox\nbsphinxbox
\makeatletter
\newcommand{\nbsphinxstartnotebook}[1]{%
    \par
    % measure needed space
    \setbox\nbsphinxbox\vtop{{#1\par}}
    % reserve some space at bottom of page, else start new page
    \needspace{\dimexpr2.5\baselineskip+\ht\nbsphinxbox+\dp\nbsphinxbox}
    % mimick vertical spacing from \section command
      \addpenalty\@secpenalty
      \@tempskipa 3.5ex \@plus 1ex \@minus .2ex\relax
      \addvspace\@tempskipa
      {\Large\@tempskipa\baselineskip
             \advance\@tempskipa-\prevdepth
             \advance\@tempskipa-\ht\nbsphinxbox
             \ifdim\@tempskipa>\z@
               \vskip \@tempskipa
             \fi}
    \unvbox\nbsphinxbox
    % if notebook starts with a \section, prevent it from adding extra space
    \@nobreaktrue\everypar{\@nobreakfalse\everypar{}}%
    % compensate the parskip which will get inserted by next paragraph
    \nobreak\vskip-\parskip
    % do not break here
    \nobreak
}% end of \nbsphinxstartnotebook

\newcommand{\nbsphinxstopnotebook}[1]{%
    \par
    % measure needed space
    \setbox\nbsphinxbox\vbox{{#1\par}}
    \nobreak % it updates page totals
    \dimen@\pagegoal
    \advance\dimen@-\pagetotal \advance\dimen@-\pagedepth
    \advance\dimen@-\ht\nbsphinxbox \advance\dimen@-\dp\nbsphinxbox
    \ifdim\dimen@<\z@
      % little space left
      \unvbox\nbsphinxbox
      \kern-.8\baselineskip
      \nobreak\vskip\z@\@plus1fil
      \penalty100
      \vskip\z@\@plus-1fil
      \kern.8\baselineskip
    \else
      \unvbox\nbsphinxbox
    \fi
}% end of \nbsphinxstopnotebook

% Ensure height of an included graphics fits in nbsphinxfancyoutput frame
\newdimen\nbsphinx@image@maxheight % set in nbsphinxfancyoutput environment
\newcommand*{\nbsphinxincludegraphics}[2][]{%
    \gdef\spx@includegraphics@options{#1}%
    \setbox\spx@image@box\hbox{\includegraphics[#1,draft]{#2}}%
    \in@false
    \ifdim \wd\spx@image@box>\linewidth
      \g@addto@macro\spx@includegraphics@options{,width=\linewidth}%
      \in@true
    \fi
    % no rotation, no need to worry about depth
    \ifdim \ht\spx@image@box>\nbsphinx@image@maxheight
      \g@addto@macro\spx@includegraphics@options{,height=\nbsphinx@image@maxheight}%
      \in@true
    \fi
    \ifin@
      \g@addto@macro\spx@includegraphics@options{,keepaspectratio}%
    \fi
    \setbox\spx@image@box\box\voidb@x % clear memory
    \expandafter\includegraphics\expandafter[\spx@includegraphics@options]{#2}%
}% end of "\MakeFrame"-safe variant of \sphinxincludegraphics
\makeatother

\makeatletter
\renewcommand*\sphinx@verbatim@nolig@list{\do\'\do\`}
\begingroup
\catcode`'=\active
\let\nbsphinx@noligs\@noligs
\g@addto@macro\nbsphinx@noligs{\let'\PYGZsq}
\endgroup
\makeatother
\renewcommand*\sphinxbreaksbeforeactivelist{\do\<\do\"\do\'}
\renewcommand*\sphinxbreaksafteractivelist{\do\.\do\,\do\:\do\;\do\?\do\!\do\/\do\>\do\-}
\makeatletter
\fvset{codes*=\sphinxbreaksattexescapedchars\do\^\^\let\@noligs\nbsphinx@noligs}
\makeatother



\title{Implementation of a real\sphinxhyphen{}time semantic retrieval system.\@{}}
\date{Jun 17, 2021}
\release{}
\author{David Lorenzo Alfaro}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


The increasingly overwhelming amount of available natural language motivates the pressing need to find efficient and reliable computational techniques capable of processing and analysing this type of data for the purpose of achieving human\sphinxhyphen{}like natural language understanding for a wide range of downstream tasks.

Over the last decade, Natural Language Processing (NLP) has seen impressively fast growth, primarily favoured by the increase in computational power and the progress on unsupervised learning in linguistics. Moreover, historically successful statistical language modeling techniques have been largely replaced by novel neural language modeling based on Deep Learning models, exhibiting an unprecedent level of natural language understanding, contributing to reduce the gap between human communication and computer understanding.

NLP is the key to solve many technological challenges. Among the large number of applications this field has, and since this dissertation has been entirely focused on the study of different strategies to encode salient information about natural language, the experimental part of this work is primarily devoted to the implementation of a semantic information retrieval system, delivering search latencies suitable for real\sphinxhyphen{}time similarity matching.

Of course, the examined unsupervised pretrained representations have been trained on humongous data sets, devoting to that end massive amounts of computational resources, something we do not have the access to.  It is, however, not only a matter of computational power.  Gathering, preparing and processing data for a model to be fine\sphinxhyphen{}tuned is no easy task and requires knowledge, to a great extent, of both the underlying theoretical motivations and the specific implementation. Consequently, our work is pretty much aligned with the mentality in which these strategies sit on: to directly use the pretrained models for downstream tasks.

This document contains the documentation for all experiments conducted throughout the dissertation, including a thoroughly description of the implementation of the information retrieval system and the notebooks devoted to the dataset analysis and preprocessing and the analysis and visualization of BERT contextual embeddings.


\chapter{Semantic search (\sphinxstyleliteralintitle{\sphinxupquote{semantic\_search.py}})}
\label{\detokenize{code:module-semantic_search}}\label{\detokenize{code:semantic-search-semantic-search-py}}\label{\detokenize{code::doc}}\index{module@\spxentry{module}!semantic\_search@\spxentry{semantic\_search}}\index{semantic\_search@\spxentry{semantic\_search}!module@\spxentry{module}}
This class implements a semantic textual information retrieval system.

Author: David Lorenzo Alfaro.
\index{SemanticSearch (class in semantic\_search)@\spxentry{SemanticSearch}\spxextra{class in semantic\_search}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:semantic_search.SemanticSearch}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{semantic\_search.}}\sphinxbfcode{\sphinxupquote{SemanticSearch}}}{\emph{\DUrole{n}{corpus}\DUrole{p}{:} \DUrole{n}{pandas.core.frame.DataFrame}}, \emph{\DUrole{n}{embeddings\_cache\_path}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{encoding\_strategy}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}title\_overview\textquotesingle{}}}, \emph{\DUrole{n}{pretrained\_model}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}paraphrase\sphinxhyphen{}distilroberta\sphinxhyphen{}base\sphinxhyphen{}v2\textquotesingle{}}}, \emph{\DUrole{n}{model\_max\_seq\_length}\DUrole{o}{=}\DUrole{default_value}{512}}, \emph{\DUrole{n}{pretrained\_crossencoder}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{annoy\_index\_cache\_path}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{annoy\_n\_trees}\DUrole{o}{=}\DUrole{default_value}{576}}, \emph{\DUrole{n}{\_annoy\_embedding\_size}\DUrole{o}{=}\DUrole{default_value}{768}}}{}
This class implements a semantic textual information retrieval system,
which allows for advanced features like retrieve and re\sphinxhyphen{}rank and Approximate
Nearest Neighbours search.
\index{\_\_init\_\_() (semantic\_search.SemanticSearch method)@\spxentry{\_\_init\_\_()}\spxextra{semantic\_search.SemanticSearch method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:semantic_search.SemanticSearch.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{corpus}\DUrole{p}{:} \DUrole{n}{pandas.core.frame.DataFrame}}, \emph{\DUrole{n}{embeddings\_cache\_path}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{encoding\_strategy}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}title\_overview\textquotesingle{}}}, \emph{\DUrole{n}{pretrained\_model}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}paraphrase\sphinxhyphen{}distilroberta\sphinxhyphen{}base\sphinxhyphen{}v2\textquotesingle{}}}, \emph{\DUrole{n}{model\_max\_seq\_length}\DUrole{o}{=}\DUrole{default_value}{512}}, \emph{\DUrole{n}{pretrained\_crossencoder}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{annoy\_index\_cache\_path}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{annoy\_n\_trees}\DUrole{o}{=}\DUrole{default_value}{576}}, \emph{\DUrole{n}{\_annoy\_embedding\_size}\DUrole{o}{=}\DUrole{default_value}{768}}}{}
Constructor for a \sphinxtitleref{SemanticSearch} instance.

\sphinxstylestrong{Important}: if you are willing to load the embeddings or the ANNOY 
index from disk you do not need to tune their respective specific 
parameters (they will be overlooked).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{corpus}} (\sphinxstyleliteralemphasis{\sphinxupquote{DataFrame}}) \textendash{} 
Book corpus. It should, at least, have the following columns        
(with the very same names):
\begin{itemize}
\item {} 
\sphinxtitleref{gr\_book\_id}: book unique identifier.

\item {} 
\sphinxtitleref{title}: book titles.

\item {} 
\sphinxtitleref{authors}: book authors.

\item {} 
\sphinxtitleref{overview}: book overview.

\end{itemize}


\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{embeddings\_cache\_path}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Filepath to store the computed embeddings
or load the embeddings from. Defaults to None.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{encoding\_strategy}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Encoding strategy to use. The encoding strategy
must be a string containing the names of the features to include into
the input of the encoder, each of them separated by an underscore (‘\_’).
For example, if you were to use the title and the overview as the encoding        
strategy, \sphinxtitleref{encoding\_strategy} must be either \sphinxtitleref{title\_overview} or \sphinxtitleref{overview\_title}.
Defaults to ‘title\_overview’.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{pretrained\_model}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The model \sphinxtitleref{id} of a predefined
\sphinxtitleref{SentenceTransformer} hosted inside a model repo on sbert.net. Defaults
to ‘paraphrase\sphinxhyphen{}distilroberta\sphinxhyphen{}base\sphinxhyphen{}v2’. Defaults to ‘paraphrase\sphinxhyphen{}distilroberta\sphinxhyphen{}base\sphinxhyphen{}v2’.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model\_max\_seq\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Property to get the maximal input sequence
length for the model. Longer inputs will be truncated. Defaults to 512.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{pretrained\_crossencoder}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Any model name from Huggingface Models
Repository that can be loaded with AutoModel. Defaults to None.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{annoy\_index\_cache\_path}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Filepath to store an ANNOY index or load
it from disk. Defaults to None.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{annoy\_n\_trees}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of trees to use in the forest for ANNOY.
Defaults to 576

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{\_annoy\_embedding\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Size of the embeddings, required to compute
the index. Defaults to 768

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{annoy\_index() (semantic\_search.SemanticSearch property)@\spxentry{annoy\_index()}\spxextra{semantic\_search.SemanticSearch property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:semantic_search.SemanticSearch.annoy_index}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{annoy\_index}}}
Getter method for \sphinxtitleref{annoy\_index}
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Current ANNOY index. If none, it attempts to create a new one
with the optimal configuration (according to the experiments detailed
in the dissertation document).

\item[{Return type}] \leavevmode
AnnoyIndex

\end{description}\end{quote}

\end{fulllineitems}

\index{crossencoder() (semantic\_search.SemanticSearch property)@\spxentry{crossencoder()}\spxextra{semantic\_search.SemanticSearch property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:semantic_search.SemanticSearch.crossencoder}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{crossencoder}}}
Getter method for \sphinxtitleref{crossencoder}.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Current pretrained Cross\sphinxhyphen{}Encoder. If none, it attempts to obtain
the default one.

\item[{Return type}] \leavevmode
CrossEncoder

\end{description}\end{quote}

\end{fulllineitems}

\index{search() (semantic\_search.SemanticSearch method)@\spxentry{search()}\spxextra{semantic\_search.SemanticSearch method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:semantic_search.SemanticSearch.search}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{search}}}{\emph{\DUrole{n}{query}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{k}\DUrole{o}{=}\DUrole{default_value}{5}}, \emph{\DUrole{n}{k\_biencoder}\DUrole{o}{=}\DUrole{default_value}{20}}, \emph{\DUrole{n}{use\_annoy}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reranking}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Perform semantic search.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{query}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Textual query.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of most relevant documents to retrieve. When using exhaustive
search, the value of \sphinxtitleref{k} does not affect perfomance. Complexity using ANNOY and
\sphinxtitleref{k} \textasciitilde{} corpus length will be close to O(n). Defaults to 5

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{k\_biencoder}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If using retrieve and re\sphinxhyphen{}rank, number of documents to
retrieve by the Bi\sphinxhyphen{}encoder and fed into the Cross\sphinxhyphen{}Encoder. The Cross\sphinxhyphen{}Encoder will
return the \sphinxtitleref{k} most relevant entries. \sphinxtitleref{k\_biencoder} must be greater or equal to
\sphinxtitleref{k}. Defaults to 20.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{use\_annoy}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Use approximate search to reduce search time to approx O(log(n)).
Defaults to False

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{reranking}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Use retrieve and Re\sphinxhyphen{}Rank Pipeline, defaults to False

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{search\_multiple() (semantic\_search.SemanticSearch method)@\spxentry{search\_multiple()}\spxextra{semantic\_search.SemanticSearch method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:semantic_search.SemanticSearch.search_multiple}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{search\_multiple}}}{\emph{\DUrole{n}{queries}\DUrole{p}{:} \DUrole{n}{list}}, \emph{\DUrole{n}{write\_to}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{o}{**}\DUrole{n}{search\_options}}}{}
Perform semantic search for several queries. Refer to the documentation
of \sphinxtitleref{search} method for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{queries}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}) \textendash{} Collection of textual queries.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{write\_to}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Path of the file in which the results of the query will be
written. If the file already exists, existing data will be overwritten.
Defaults to None.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{setup\_annoy() (semantic\_search.SemanticSearch method)@\spxentry{setup\_annoy()}\spxextra{semantic\_search.SemanticSearch method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:semantic_search.SemanticSearch.setup_annoy}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{setup\_annoy}}}{\emph{\DUrole{n}{index\_cache\_path}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{n\_trees}\DUrole{o}{=}\DUrole{default_value}{576}}, \emph{\DUrole{n}{embedding\_size}\DUrole{o}{=}\DUrole{default_value}{768}}}{}
Setup for ANNOY index. Use this method to:
\begin{itemize}
\item {} 
Use a precomputed ANNOY index located in \sphinxtitleref{index\_cache\_path}.

\item {} \begin{description}
\item[{Create a new ANNOY index and store it in \sphinxtitleref{index\_cache\_path}. }] \leavevmode
if \sphinxtitleref{index\_cache\_path} is \sphinxtitleref{None}, the index will not be stored in disk.

\end{description}

\item {} \begin{description}
\item[{Either way, the obtained ANNOY index will be used in future calls }] \leavevmode
to \sphinxtitleref{search} and \sphinxtitleref{search\_multiple} if approximate search is chosen.

\end{description}

\item {} 
Previous ANNOY setup is replaced upon invoking this method.

\end{itemize}

\sphinxstylestrong{IMPORTANT}: if you are attempting to load an ANNOY index from disk, there
is no need to tune the remaining parameters (i.e., \sphinxtitleref{n\_trees} and 
\sphinxtitleref{embedding\_size})
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{index\_cache\_path}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Filepath to store the obtained ANNOY index or filepath
of a precomputed ANNOY index. By default is \sphinxtitleref{None}: a new ANNOY index will be
created with the indicated parameters.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{n\_trees}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of trees to use in the forest for ANNOY, defaults to 576.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{embedding\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Size of the embeddings, required to compute the index.
Defaults to 768

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{setup\_crossencoder() (semantic\_search.SemanticSearch method)@\spxentry{setup\_crossencoder()}\spxextra{semantic\_search.SemanticSearch method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:semantic_search.SemanticSearch.setup_crossencoder}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{setup\_crossencoder}}}{\emph{\DUrole{n}{pretrained\_crossencoder}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}cross\sphinxhyphen{}encoder/stsb\sphinxhyphen{}distilroberta\sphinxhyphen{}base\textquotesingle{}}}}{}
Set or update the Cross\sphinxhyphen{}Encoder to be used to re\sphinxhyphen{}rank the results 
retrieved by Bi\sphinxhyphen{}Encoder. We recomend using either 
‘cross\sphinxhyphen{}encoder/stsb\sphinxhyphen{}distilroberta\sphinxhyphen{}base’ or any pretrained Cross\sphinxhyphen{}Encoder
trained on  MS MARCO dataset.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{pretrained\_crossencoder}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Any model name from Huggingface Models 
Repository that can be loaded with AutoModel. Defaults to 
‘cross\sphinxhyphen{}encoder/stsb\sphinxhyphen{}distilroberta\sphinxhyphen{}base’

\end{description}\end{quote}

\end{fulllineitems}

\index{test\_annoy\_performance() (semantic\_search.SemanticSearch method)@\spxentry{test\_annoy\_performance()}\spxextra{semantic\_search.SemanticSearch method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:semantic_search.SemanticSearch.test_annoy_performance}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{test\_annoy\_performance}}}{\emph{\DUrole{n}{queries}\DUrole{p}{:} \DUrole{n}{list}}, \emph{\DUrole{n}{k}\DUrole{o}{=}\DUrole{default_value}{5}}, \emph{\DUrole{n}{verbose}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Utility to test the performance of ANNOY, considering the speedup with respect to
exhaustive search and the recall.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{queries}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Collection of textual queries.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Top k elements to consider in the comparison.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Lexical search (\sphinxstyleliteralintitle{\sphinxupquote{lexical\_search.py}})}
\label{\detokenize{code:module-lexical_search}}\label{\detokenize{code:lexical-search-lexical-search-py}}\index{module@\spxentry{module}!lexical\_search@\spxentry{lexical\_search}}\index{lexical\_search@\spxentry{lexical\_search}!module@\spxentry{module}}
This class implements a textual literal information retrieval system.

Author: David Lorenzo Alfaro.
\index{TfIdfSearch (class in lexical\_search)@\spxentry{TfIdfSearch}\spxextra{class in lexical\_search}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:lexical_search.TfIdfSearch}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{lexical\_search.}}\sphinxbfcode{\sphinxupquote{TfIdfSearch}}}{\emph{\DUrole{n}{corpus}\DUrole{p}{:} \DUrole{n}{pandas.core.frame.DataFrame}}, \emph{\DUrole{n}{vectors\_cache\_path}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{encoding\_strategy}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}title\_overview\textquotesingle{}}}, \emph{\DUrole{n}{pretrained\_biencoder}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{embeddings\_cache\_path}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{biencoder\_max\_seq\_length}\DUrole{o}{=}\DUrole{default_value}{512}}, \emph{\DUrole{n}{pretrained\_crossencoder}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}}{}
This class implements a textual literal information retrieval system,
based on TF\sphinxhyphen{}IDF which allows for advanced features like hybrid search,
combining literal and dense search (retrieve and re\sphinxhyphen{}rank search pipeline).
\index{\_\_init\_\_() (lexical\_search.TfIdfSearch method)@\spxentry{\_\_init\_\_()}\spxextra{lexical\_search.TfIdfSearch method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:lexical_search.TfIdfSearch.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{corpus}\DUrole{p}{:} \DUrole{n}{pandas.core.frame.DataFrame}}, \emph{\DUrole{n}{vectors\_cache\_path}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{encoding\_strategy}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}title\_overview\textquotesingle{}}}, \emph{\DUrole{n}{pretrained\_biencoder}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{embeddings\_cache\_path}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{biencoder\_max\_seq\_length}\DUrole{o}{=}\DUrole{default_value}{512}}, \emph{\DUrole{n}{pretrained\_crossencoder}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}}{}
Constructor for a \sphinxtitleref{TfIdfSearch} instance.

\sphinxstylestrong{Important}: if you are willing to load the embeddings or the vectors
from disk you do not need to tune their respective specific parameters 
(they will be overlooked).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{corpus}} (\sphinxstyleliteralemphasis{\sphinxupquote{DataFrame}}) \textendash{} 
Book corpus. It should, at least, have the following columns        
(with the very same names):
\begin{itemize}
\item {} 
\sphinxtitleref{gr\_book\_id}: book unique identifier.

\item {} 
\sphinxtitleref{title}: book titles.

\item {} 
\sphinxtitleref{authors}: book authors.

\item {} 
\sphinxtitleref{overview}: book overview.

\end{itemize}


\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{vectors\_cache\_path}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Filepath to store the computed vectors
or load the vectors from. Defaults to None.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{encoding\_strategy}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Encoding strategy to use. The encoding strategy
must be a string containing the names of the features to include into
the input of the encoder, each of them separated by an underscore (‘\_’).
For example, if you were to use the title and the overview as the encoding        
strategy, \sphinxtitleref{encoding\_strategy} must be either \sphinxtitleref{title\_overview} or \sphinxtitleref{overview\_title}.
Defaults to ‘title\_overview’.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{pretrained\_biencoder}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The model \sphinxtitleref{id} of a predefined
\sphinxtitleref{SentenceTransformer} hosted inside a model repo on sbert.net. Defaults
to ‘paraphrase\sphinxhyphen{}distilroberta\sphinxhyphen{}base\sphinxhyphen{}v2’. Defaults to None.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{embeddings\_cache\_path}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Filepath to store the computed embeddings
or load the embeddings from. Defaults to None.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{biencoder\_max\_seq\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Property to get the maximal input sequence
length for the model. Longer inputs will be truncated. Defaults to 512.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{pretrained\_crossencoder}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Any model name from Huggingface Models
Repository that can be loaded with AutoModel. Defaults to None.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{biencoder() (lexical\_search.TfIdfSearch property)@\spxentry{biencoder()}\spxextra{lexical\_search.TfIdfSearch property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:lexical_search.TfIdfSearch.biencoder}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{biencoder}}}
Getter method for \sphinxtitleref{biencoder}.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Current pretrained Bi\sphinxhyphen{}Encoder. If none, it attempts to obtain
the default one.

\item[{Return type}] \leavevmode
SentenceTransformer

\end{description}\end{quote}

\end{fulllineitems}

\index{crossencoder() (lexical\_search.TfIdfSearch property)@\spxentry{crossencoder()}\spxextra{lexical\_search.TfIdfSearch property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:lexical_search.TfIdfSearch.crossencoder}}\pysigline{\sphinxbfcode{\sphinxupquote{property }}\sphinxbfcode{\sphinxupquote{crossencoder}}}
Getter method for \sphinxtitleref{crossencoder}.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Current pretrained Cross\sphinxhyphen{}Encoder. If none, it attempts to obtain
the default one.

\item[{Return type}] \leavevmode
CrossEncoder

\end{description}\end{quote}

\end{fulllineitems}

\index{search() (lexical\_search.TfIdfSearch method)@\spxentry{search()}\spxextra{lexical\_search.TfIdfSearch method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:lexical_search.TfIdfSearch.search}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{search}}}{\emph{\DUrole{n}{query}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{k}\DUrole{o}{=}\DUrole{default_value}{5}}, \emph{\DUrole{n}{k\_lexical}\DUrole{o}{=}\DUrole{default_value}{20}}, \emph{\DUrole{n}{reranking\_strategy}\DUrole{p}{:} \DUrole{n}{Literal\DUrole{p}{{[}}{\hyperref[\detokenize{code:lexical_search.TfIdfSearch.crossencoder}]{\sphinxcrossref{crossencoder}}}\DUrole{p}{, }{\hyperref[\detokenize{code:lexical_search.TfIdfSearch.biencoder}]{\sphinxcrossref{biencoder}}}\DUrole{p}{{]}}} \DUrole{o}{=} \DUrole{default_value}{None}}}{}
Perform TF\sphinxhyphen{}IDF lexical search.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{query}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Textual query.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of most relevant documents to retrieve.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{k\_lexical}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If using retrieve and re\sphinxhyphen{}rank, number of documents to
retrieve by lexical search and re\sphinxhyphen{}ranked by any re\sphinxhyphen{}ranking strategy. Re\sphinxhyphen{}Ranker
will return the \sphinxtitleref{k} most relevant entries. \sphinxtitleref{k\_lexical} must be greater or equal
to \sphinxtitleref{k}. Defaults to 20.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{reranking\_strategy}} (\sphinxstyleliteralemphasis{\sphinxupquote{Literal}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{\textquotesingle{}crossencoder\textquotesingle{}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{\textquotesingle{}biencoder\textquotesingle{}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Re\sphinxhyphen{}ranking strategy to use. Defaults to None

\end{itemize}

\item[{Raises}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{ValueError}} \textendash{} Raise \sphinxtitleref{ValueError} if \sphinxtitleref{reranking\_strategy} takes an ilegal
value.

\end{description}\end{quote}

\end{fulllineitems}

\index{search\_multiple() (lexical\_search.TfIdfSearch method)@\spxentry{search\_multiple()}\spxextra{lexical\_search.TfIdfSearch method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:lexical_search.TfIdfSearch.search_multiple}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{search\_multiple}}}{\emph{\DUrole{n}{queries}\DUrole{p}{:} \DUrole{n}{list}}, \emph{\DUrole{n}{write\_to}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{o}{**}\DUrole{n}{search\_options}}}{}
Perform lexical search for several queries. Refer to the documentation
of \sphinxtitleref{search} method for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{queries}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}) \textendash{} Collection of textual queries.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{write\_to}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Path of the file in which the results of the query will be
written. If the file already exists, existing data will be overwritten.
Defaults to None.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{setup\_biencoder() (lexical\_search.TfIdfSearch method)@\spxentry{setup\_biencoder()}\spxextra{lexical\_search.TfIdfSearch method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:lexical_search.TfIdfSearch.setup_biencoder}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{setup\_biencoder}}}{\emph{\DUrole{n}{pretrained\_biencoder}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}paraphrase\sphinxhyphen{}distilroberta\sphinxhyphen{}base\sphinxhyphen{}v2\textquotesingle{}}}, \emph{\DUrole{n}{max\_seq\_length}\DUrole{o}{=}\DUrole{default_value}{512}}}{}
Set or update the Bi\sphinxhyphen{}Encoder to be used to re\sphinxhyphen{}rank the results 
retrieved by TF\sphinxhyphen{}IDF search.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{pretrained\_model}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The model \sphinxtitleref{id} of a predefined
\sphinxtitleref{SentenceTransformer} hosted inside a model repo on sbert.net. Defaults
to ‘paraphrase\sphinxhyphen{}distilroberta\sphinxhyphen{}base\sphinxhyphen{}v2’. Defaults to ‘paraphrase\sphinxhyphen{}distilroberta\sphinxhyphen{}base\sphinxhyphen{}v2’.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{max\_seq\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Property to get the maximal input sequence
length for the model. Longer inputs will be truncated. Defaults to 512.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{setup\_crossencoder() (lexical\_search.TfIdfSearch method)@\spxentry{setup\_crossencoder()}\spxextra{lexical\_search.TfIdfSearch method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:lexical_search.TfIdfSearch.setup_crossencoder}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{setup\_crossencoder}}}{\emph{\DUrole{n}{pretrained\_crossencoder}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}cross\sphinxhyphen{}encoder/stsb\sphinxhyphen{}distilroberta\sphinxhyphen{}base\textquotesingle{}}}}{}
Set or update the Cross\sphinxhyphen{}Encoder to be used to re\sphinxhyphen{}rank the results 
retrieved by TF\sphinxhyphen{}IDF search. We recomend using either 
‘cross\sphinxhyphen{}encoder/stsb\sphinxhyphen{}distilroberta\sphinxhyphen{}base’ or any pretrained Cross\sphinxhyphen{}Encoder
trained on  MS MARCO dataset.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{pretrained\_crossencoder}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Any model name from Huggingface Models 
Repository that can be loaded with AutoModel. Defaults to 
‘cross\sphinxhyphen{}encoder/stsb\sphinxhyphen{}distilroberta\sphinxhyphen{}base’

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Code utilities}
\label{\detokenize{code:code-utilities}}
Documentation for the \sphinxtitleref{code\_utils} Python package.


\section{Utils (\sphinxstyleliteralintitle{\sphinxupquote{utils.py}})}
\label{\detokenize{code:module-code_utils.utils}}\label{\detokenize{code:utils-utils-py}}\index{module@\spxentry{module}!code\_utils.utils@\spxentry{code\_utils.utils}}\index{code\_utils.utils@\spxentry{code\_utils.utils}!module@\spxentry{module}}
Set of miscellaneous utilities used across the implementation.

Author: David Lorenzo Alfaro.
\index{append\_overviews\_to\_data() (in module code\_utils.utils)@\spxentry{append\_overviews\_to\_data()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.append_overviews_to_data}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{append\_overviews\_to\_data}}}{\emph{\DUrole{n}{df\_overviews}\DUrole{p}{:} \DUrole{n}{pandas.core.frame.DataFrame}}, \emph{\DUrole{n}{df\_data}\DUrole{p}{:} \DUrole{n}{pandas.core.frame.DataFrame}}, \emph{\DUrole{n}{overview\_index}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{\textquotesingle{}gr\_book\_id\textquotesingle{}}}, \emph{\DUrole{n}{data\_index}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{\textquotesingle{}gr\_book\_id\textquotesingle{}}}, \emph{\DUrole{n}{merge\_option}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}right\textquotesingle{}}}}{}
Merge \sphinxtitleref{df\_overviews} with \sphinxtitleref{df\_data} using column identifiers \sphinxtitleref{overview\_index} and
\sphinxtitleref{data\_index}, respectively. We allow books with no overviews, hence \sphinxstyleemphasis{right join}
is the most suitable operation.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{df\_overviews}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} Book overviews.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{df\_data}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} Book data (e.g., title, authors, etc.)

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{overview\_index}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Column or index level names to join on in the left DataFrame,
defaults to ‘gr\_book\_id’.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_index}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Column or index level names to join on in the right DataFrame,
defaults to ‘gr\_book\_id’.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{merge\_option}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Type of merge operation, to be performed can be one of \{‘left’,
‘right’, ‘outer’, ‘inner’\}, to ‘right’.

\end{itemize}

\item[{Returns}] \leavevmode
A DataFrame of the two merged objects.

\item[{Return type}] \leavevmode
pd.DataFrame

\end{description}\end{quote}

\end{fulllineitems}

\index{clean\_book\_title() (in module code\_utils.utils)@\spxentry{clean\_book\_title()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.clean_book_title}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{clean\_book\_title}}}{\emph{\DUrole{n}{title}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{remove\_quotation\_marks}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{remove\_saga\_info}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{remove\_saga\_number}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Applies several transformations to a book title to remove noisy data that
can potentially affect the performance of the embedding strategies.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{title}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Book title in plain text.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{remove\_quotation\_marks}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If set to True, attempts to remove the quotation
marks enclosing the book title, to True.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{remove\_saga\_info}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If set to True, attempts to remove information concerning
the book saga, defaults to False.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{remove\_saga\_number}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If set to True, attempts to remove the saga number,
defaults to True.

\end{itemize}

\item[{Returns}] \leavevmode
Processed book title.

\item[{Return type}] \leavevmode
str

\end{description}\end{quote}

\end{fulllineitems}

\index{clean\_overview() (in module code\_utils.utils)@\spxentry{clean\_overview()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.clean_overview}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{clean\_overview}}}{\emph{\DUrole{n}{overview}\DUrole{p}{:} \DUrole{n}{str}}}{}
Applies several transformations to a book overview to remove noisy data 
that can potentially affect the performance of the embedding strategies.
One must be careful when applying transformations to the whole corpus because
the odds for negative side\sphinxhyphen{}effects are high. Here, we attempt to solve some of the
problems spotted that, in our tests, should not have any noticeable negative effect
on any book overview.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{overview}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Book overview in plain text.

\item[{Returns}] \leavevmode
Processed book overview.

\item[{Return type}] \leavevmode
str

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_avg\_wordpiece\_tokens() (in module code\_utils.utils)@\spxentry{compute\_avg\_wordpiece\_tokens()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.compute_avg_wordpiece_tokens}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{compute\_avg\_wordpiece\_tokens}}}{\emph{\DUrole{n}{corpus}\DUrole{p}{:} \DUrole{n}{list}}, \emph{\DUrole{n}{tokenizer}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
Compute the average number of WordPiece tokens in a list of documents, \sphinxtitleref{corpus}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{corpus}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} List of textual documents.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{tokenizer}} (\sphinxstyleliteralemphasis{\sphinxupquote{BertTokenizer}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instance of \sphinxtitleref{BertTokenizer} class. If \sphinxtitleref{None}, it loads the
predefined tokenizer of ‘bert\sphinxhyphen{}base\sphinxhyphen{}uncased’.

\end{itemize}

\item[{Returns}] \leavevmode
Average number of WordPiece tokens of the documents in \sphinxtitleref{corpus}.

\item[{Return type}] \leavevmode
int

\end{description}\end{quote}

\end{fulllineitems}

\index{fix\_punctuation() (in module code\_utils.utils)@\spxentry{fix\_punctuation()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.fix_punctuation}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{fix\_punctuation}}}{\emph{\DUrole{n}{overview}\DUrole{p}{:} \DUrole{n}{str}}}{}
Attempts to fix some of the identified punctuation issues present in the 
book overviews.
\begin{description}
\item[{It is a common issue to find overviews with the following punctuation flaw:}] \leavevmode\begin{itemize}
\item {} 
“{[}…{]} word.Word {[}…{]}”

\item {} 
“{[}…{]} word!Word {[}…{]}”

\item {} 
“{[}…{]} word?Word {[}…{]}”

\end{itemize}

\end{description}

That is to say, spacing after periods, exclamation and question marks is not
correctly applied. This lead to some issues when splitting the text into sentences,
specially using the NLTK library. Furthermore, it may have other adverse effects
on the embedding process (e.g., due to faulty tokenization).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{overview}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Book overview in plain text.

\item[{Returns}] \leavevmode
str

\item[{Return type}] \leavevmode
Book overview without the identified punctuation flaws.

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_dataframe\_from\_sparse\_txts() (in module code\_utils.utils)@\spxentry{generate\_dataframe\_from\_sparse\_txts()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.generate_dataframe_from_sparse_txts}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{generate\_dataframe\_from\_sparse\_txts}}}{\emph{\DUrole{n}{base\_dir}}, \emph{\DUrole{n}{path\_standard\_format}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{out\_filename}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
Generates a dataframe from all \sphinxstyleemphasis{txt} files located in \sphinxtitleref{base\_dir}. The dataframe 
features two columns: \sphinxtitleref{gr\_book\_id}, an identifier that is retrieved from the name of
each \sphinxstyleemphasis{txt} file, and \sphinxtitleref{overview}, containing all information included in the \sphinxstyleemphasis{txt} file.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{base\_dir}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Directory in which the \sphinxstyleemphasis{txt} files for the book overviews
are located.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{path\_standard\_format}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicates whether the path follows the standard
format (backslash separator) or the slash separator, defaults to False.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{out\_filename}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Path for the output file, defaults to None.

\end{itemize}

\item[{Returns}] \leavevmode
Returns a dataframe from all txt files located in \sphinxtitleref{base\_dir}.

\item[{Return type}] \leavevmode
pd.DataFrame

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_bert\_model() (in module code\_utils.utils)@\spxentry{get\_bert\_model()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.get_bert_model}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{get\_bert\_model}}}{\emph{\DUrole{n}{transformer}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}bert\sphinxhyphen{}base\sphinxhyphen{}uncased\textquotesingle{}}}}{}
Get an instance of the class \sphinxtitleref{BertModel} for the transformer \sphinxtitleref{trasformer}.

Wrapper function of the HuggingFace Transformer’s \sphinxtitleref{BertModel} function:
\sphinxtitleref{from\_pretrained}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{transformer}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The model \sphinxtitleref{id} of a predefined tokenizer hosted inside a
model repo on huggingface.co. Defaults to ‘bert\sphinxhyphen{}base\sphinxhyphen{}uncased’

\end{description}\end{quote}

:return Instance of \sphinxtitleref{BertTokenizer} class.
:rtype: BertTokenizer

\end{fulllineitems}

\index{get\_bert\_tokenizer() (in module code\_utils.utils)@\spxentry{get\_bert\_tokenizer()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.get_bert_tokenizer}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{get\_bert\_tokenizer}}}{\emph{\DUrole{n}{transformer}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}bert\sphinxhyphen{}base\sphinxhyphen{}uncased\textquotesingle{}}}}{}
Get an instance of the class \sphinxtitleref{BertTokenizer} for the transformer \sphinxtitleref{trasformer}.

Wrapper function of the HuggingFace Transformer’s \sphinxtitleref{BertTokenizer} function:
\sphinxtitleref{from\_pretrained}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{transformer}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The model \sphinxtitleref{id} of a predefined tokenizer hosted inside a
model repo on huggingface.co. Defaults to ‘bert\sphinxhyphen{}base\sphinxhyphen{}uncased’

\end{description}\end{quote}

:return Instance of \sphinxtitleref{BertTokenizer} class.
:rtype: BertTokenizer

\end{fulllineitems}

\index{get\_crossencoder() (in module code\_utils.utils)@\spxentry{get\_crossencoder()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.get_crossencoder}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{get\_crossencoder}}}{\emph{\DUrole{n}{crossencoder}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}cross\sphinxhyphen{}encoder/stsb\sphinxhyphen{}distilroberta\sphinxhyphen{}base\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
Wrapper function to get a \sphinxtitleref{CrossEncoder}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{crossencoder}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Any model name from Huggingface Models repository that can
be loaded with AutoModel. Defaults to ‘cross\sphinxhyphen{}encoder/stsb\sphinxhyphen{}distilroberta\sphinxhyphen{}base’.

\item[{Returns}] \leavevmode
a CrossEncoder that takes exactly two sentences/texts as input and predicts
a score for this sentence pair.It can for example predict the similarity of the
sentence pair on a scale of 0 … 1.

\item[{Return type}] \leavevmode
CrossEncoder

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_dir\_files\_content() (in module code\_utils.utils)@\spxentry{get\_dir\_files\_content()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.get_dir_files_content}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{get\_dir\_files\_content}}}{\emph{\DUrole{n}{directory}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{file\_extension}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}.txt\textquotesingle{}}}}{}
Get the list of files in \sphinxtitleref{directory} with extension \sphinxtitleref{file\_extension}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{directory}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Directory in which the files are located. Recursive search
is not allowed.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{file\_extension}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} File extension, defaults to ‘.txt’

\end{itemize}

\item[{Returns}] \leavevmode
List of tuples (filename, file content).

\item[{Return type}] \leavevmode
list

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_file\_id\_and\_content() (in module code\_utils.utils)@\spxentry{get\_file\_id\_and\_content()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.get_file_id_and_content}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{get\_file\_id\_and\_content}}}{\emph{\DUrole{n}{filepath}}}{}
Returns all textual content in\textasciigrave{}filepath\textasciigrave{}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{filepath}} \textendash{} Path of the text file to read.

\item[{Returns}] \leavevmode
Content of the file.

\item[{Return type}] \leavevmode
str

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_sentence\_transformer() (in module code\_utils.utils)@\spxentry{get\_sentence\_transformer()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.get_sentence_transformer}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{get\_sentence\_transformer}}}{\emph{\DUrole{n}{transformer}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}paraphrase\sphinxhyphen{}distilroberta\sphinxhyphen{}base\sphinxhyphen{}v2\textquotesingle{}}}, \emph{\DUrole{n}{max\_seq\_length}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
Wrapper function to get a \sphinxtitleref{SentenceTransformer}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{transformer}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The model \sphinxtitleref{id} of a predefined \sphinxtitleref{SentenceTransformer} hosted
inside a model repo on sbert.net. Defaults to ‘paraphrase\sphinxhyphen{}distilroberta\sphinxhyphen{}base\sphinxhyphen{}v2’.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{max\_seq\_length}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Property to get the maximal input sequence length
for the model. Longer inputs will be truncated. Defaults to None.

\end{itemize}

\item[{Returns}] \leavevmode
a SentenceTransformer model that can be used to map sentences / text
to embeddings.

\item[{Return type}] \leavevmode
SentenceTransformer

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_tokenized\_text() (in module code\_utils.utils)@\spxentry{get\_tokenized\_text()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.get_tokenized_text}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{get\_tokenized\_text}}}{\emph{\DUrole{n}{text}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{tokenizer}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
Get the list of WordPiece tokens in \sphinxtitleref{text}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{text}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Text to tokenize

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{tokenizer}} (\sphinxstyleliteralemphasis{\sphinxupquote{BertTokenizer}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instance of \sphinxtitleref{BertTokenizer} class. If \sphinxtitleref{None}, it loads the
predefined tokenizer of ‘bert\sphinxhyphen{}base\sphinxhyphen{}uncased’.

\end{itemize}

\item[{Returns}] \leavevmode
list of WordPiece tokens.

\item[{Return type}] \leavevmode
list

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_top\_k\_sentences() (in module code\_utils.utils)@\spxentry{get\_top\_k\_sentences()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.get_top_k_sentences}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{get\_top\_k\_sentences}}}{\emph{\DUrole{n}{document}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{k}\DUrole{o}{=}\DUrole{default_value}{5}}, \emph{\DUrole{n}{embedder}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{spacy\_model}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{preserve\_order}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Get the top \sphinxtitleref{k} most meaningful sentences of \sphinxtitleref{document}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{document}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} a document in plain text

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Top k sentences to return, defaults to 5

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{embedder}} (\sphinxstyleliteralemphasis{\sphinxupquote{SentenceTransformer}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instance of \sphinxtitleref{SentenceTransformer}. If \sphinxtitleref{None}, it loads the 
default pretrained sentence transformer model. Defaults to None.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{spacy\_model}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Name of a spacy pretrained tokenizer model. If \sphinxtitleref{None}, it
loads the default model. Defaults to None.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{preserve\_order}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Preserve the order of the top k sentences with respect
to the original document to conserve spatial dependencies between sentences.
Defaults to True.

\end{itemize}

\item[{Returns}] \leavevmode
Top \sphinxtitleref{k} most meaningful sentences of \sphinxtitleref{document}.

\item[{Return type}] \leavevmode
list

\end{description}\end{quote}

\end{fulllineitems}

\index{load\_corpus() (in module code\_utils.utils)@\spxentry{load\_corpus()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.load_corpus}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{load\_corpus}}}{\emph{\DUrole{n}{path\_corpus}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{sep}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{},\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
Wrapper method of Pandas \sphinxtitleref{read\_csv} function to load book corpus.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{path\_corpus}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Filepath to the corpus.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{sep}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Delimiter to use, defaults to ‘,’

\end{itemize}

\item[{Returns}] \leavevmode
Corpus DataFrame

\item[{Return type}] \leavevmode
DataFrame

\end{description}\end{quote}

\end{fulllineitems}

\index{load\_embeddings() (in module code\_utils.utils)@\spxentry{load\_embeddings()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.load_embeddings}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{load\_embeddings}}}{\emph{\DUrole{n}{filename}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{return\_dict\_values}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Utility to load embeddings (and other optional stored values) from disk
using \sphinxstyleemphasis{pickle}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{filename}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Filename of the file to be loaded.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{return\_dict\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If set to True, returns the values just the values
of the dictionary containing all stored data, defaults to True.

\end{itemize}

\item[{Returns}] \leavevmode
Loaded data

\end{description}\end{quote}

\end{fulllineitems}

\index{makedir() (in module code\_utils.utils)@\spxentry{makedir()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.makedir}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{makedir}}}{\emph{\DUrole{n}{path}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{remove\_filename}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{recursive}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{exist\_ok}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Creates directory from path if not exists.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{path}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Path of the directory to be created.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{remove\_filename}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If set to True, it attempts to remove the filename from
the path, defaults to False

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{recursive}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Creates directories recursively (i.e., create necessary 
subdirectories if necessary), defaults to True

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{exist\_ok}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} is set to False, arises an error if \sphinxtitleref{path} directory exists,
defaults to True

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{prepare\_input\_encoder() (in module code\_utils.utils)@\spxentry{prepare\_input\_encoder()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.prepare_input_encoder}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{prepare\_input\_encoder}}}{\emph{\DUrole{n}{encoding\_strategy}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{corpus}\DUrole{p}{:} \DUrole{n}{pandas.core.frame.DataFrame}}, \emph{\DUrole{n}{return\_input\_encoder}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Formats the input to the encoder using the features indicated in
\sphinxtitleref{encoding\_strategy}. If \sphinxtitleref{encoding\_strategy} takes a wrong value this method
is likely to fail. Current supported features are ‘title’, ‘authors’ and
‘overview’.
:param str encoding\_strategy: The encoding strategy must be a string containing
the names of the features to include into the input of the encoder, each of them
separated by an underscore (‘\_’). For example, if you were to use the title and
the overview as the encoding strategy, \sphinxtitleref{encoding\_strategy} must be either
\sphinxtitleref{title\_overview} or \sphinxtitleref{overview\_title}.
:param str path\_df: Path in which the dataframe is located.
:param return\_input\_encoder: Return just the collection of inputs to the encoder,
defaults to True
:type return\_input\_encoder: bool, optional
:return: If \sphinxtitleref{return\_input\_encoder}, returns the collection of inputs to the encoder.
Otherwise, it returns Dataframe including a new column \sphinxtitleref{input\_encoder} with the
format indicated in \sphinxtitleref{encoding\_strategy}
:rtype: Dataframe

\end{fulllineitems}

\index{remove\_filename\_from\_path() (in module code\_utils.utils)@\spxentry{remove\_filename\_from\_path()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.remove_filename_from_path}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{remove\_filename\_from\_path}}}{\emph{\DUrole{n}{out\_filename}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{path\_standard\_format}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
Attempts to remove filename from the provided path.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{out\_filename}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Filepath.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{path\_standard\_format}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicates whether the path follows the standard
format (backslash separator) or the slash separator, defaults to False.

\end{itemize}

\item[{Returns}] \leavevmode
The directory excluding the filename.

\item[{Return type}] \leavevmode
str

\end{description}\end{quote}

\end{fulllineitems}

\index{split\_text\_into\_sentences\_nltk() (in module code\_utils.utils)@\spxentry{split\_text\_into\_sentences\_nltk()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.split_text_into_sentences_nltk}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{split\_text\_into\_sentences\_nltk}}}{\emph{\DUrole{n}{text}\DUrole{p}{:} \DUrole{n}{str}}}{}
Splits text into sentences using the NLTK library.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{text}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Text to be splitted into sentences.

\item[{Returns}] \leavevmode
List of sentences in \sphinxtitleref{text}.

\item[{Return type}] \leavevmode
list

\end{description}\end{quote}

\end{fulllineitems}

\index{split\_text\_into\_sentences\_spacy() (in module code\_utils.utils)@\spxentry{split\_text\_into\_sentences\_spacy()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.split_text_into_sentences_spacy}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{split\_text\_into\_sentences\_spacy}}}{\emph{\DUrole{n}{text}}, \emph{\DUrole{n}{spacy\_model}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}en\_core\_web\_sm\textquotesingle{}}}}{}
Splits text into sentences using the Spacy library. SpaCy builds a syntactic
tree for each sentence, a robust method that yields more statistical information
about the text than NLTK. It performs substancially better than NLTK when using 
not polished text.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{text}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Text to be splitted into sentences.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{spacy\_model}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Name of the spacy pretrained model used to split text into
sentences, defaults to ‘en\_core\_web\_sm’.

\end{itemize}

\item[{Returns}] \leavevmode
List of sentences in \sphinxtitleref{text}.

\item[{Return type}] \leavevmode
list

\end{description}\end{quote}

\end{fulllineitems}

\index{store\_embeddings() (in module code\_utils.utils)@\spxentry{store\_embeddings()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.store_embeddings}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{store\_embeddings}}}{\emph{\DUrole{n}{corpus\_embeddings}}, \emph{\DUrole{n}{out\_filename}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}embeddings.pkl\textquotesingle{}}}, \emph{\DUrole{n}{protocol}\DUrole{o}{=}\DUrole{default_value}{5}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
Utility to dump embeddings (and other optional values indicated in the 
keyword arguments) to disk using \sphinxstyleemphasis{pickle}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{corpus\_embeddings}} \textendash{} Tensor type data structure containing the embeddings
for the corpus.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{out\_filename}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Path for the output file, defaults to ‘embeddings.pkl’.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{protocol}} \textendash{} Protocol used for \sphinxstyleemphasis{pickle}, defaults to \sphinxtitleref{pickle.HIGHEST\_PROTOCOL}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{summarize\_corpus\_overviews() (in module code\_utils.utils)@\spxentry{summarize\_corpus\_overviews()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.summarize_corpus_overviews}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{summarize\_corpus\_overviews}}}{\emph{\DUrole{n}{corpus\_overviews}\DUrole{p}{:} \DUrole{n}{list}}, \emph{\DUrole{n}{top\_k}\DUrole{o}{=}\DUrole{default_value}{5}}, \emph{\DUrole{n}{embedder}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{spacy\_model}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
Apply unsupervised Text Summarization techniques to obtain representations for
the most meaningful sentences for each document in \sphinxtitleref{corpus\_overviews}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{corpus\_overviews}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Book overviews.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{top\_k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sentences that will have each overview. Defaults to 5.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{embedder}} (\sphinxstyleliteralemphasis{\sphinxupquote{SentenceTransformer}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instance of \sphinxtitleref{SentenceTransformer}. If \sphinxtitleref{None}, it loads the default
pretrained sentence transformer model. Defaults to None.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{spacy\_model}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Name of a spacy pretrained tokenizer model. If \sphinxtitleref{None}, it
loads the default model. Defaults to None.

\end{itemize}

\item[{Returns}] \leavevmode
Summarized overviews with at most \sphinxtitleref{top\_k} sentences.

\item[{Return type}] \leavevmode
list

\end{description}\end{quote}

\end{fulllineitems}

\index{topk\_cos\_sim() (in module code\_utils.utils)@\spxentry{topk\_cos\_sim()}\spxextra{in module code\_utils.utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.utils.topk_cos_sim}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.utils.}}\sphinxbfcode{\sphinxupquote{topk\_cos\_sim}}}{\emph{\DUrole{n}{query\_embedding}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}, \emph{\DUrole{n}{embeddings}\DUrole{p}{:} \DUrole{n}{torch.Tensor}}, \emph{\DUrole{n}{top\_k}\DUrole{p}{:} \DUrole{n}{int}}}{}
Get the indices and the cosine similarity score of the \sphinxtitleref{top\_k} most similar embeddings
to \sphinxtitleref{query\_embedding} in \sphinxtitleref{embeddings}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{query\_embedding}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}) \textendash{} Query embedding.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{embeddings}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}) \textendash{} Corpus embeddings.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{top\_k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Top k most similar to retrieve according to cosine similarity
score.

\end{itemize}

\item[{Returns}] \leavevmode
List of scores and list of indexes of the top k results.

\item[{Return type}] \leavevmode
(list, list)

\end{description}\end{quote}

\end{fulllineitems}



\section{Plotter (\sphinxstyleliteralintitle{\sphinxupquote{plotter.py}})}
\label{\detokenize{code:module-code_utils.plotter}}\label{\detokenize{code:plotter-plotter-py}}\index{module@\spxentry{module}!code\_utils.plotter@\spxentry{code\_utils.plotter}}\index{code\_utils.plotter@\spxentry{code\_utils.plotter}!module@\spxentry{module}}
Set of plotting utilities used across the implementation.

Author: David Lorenzo Alfaro.
\index{apply\_dimensionality\_reduction() (in module code\_utils.plotter)@\spxentry{apply\_dimensionality\_reduction()}\spxextra{in module code\_utils.plotter}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.plotter.apply_dimensionality_reduction}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.plotter.}}\sphinxbfcode{\sphinxupquote{apply\_dimensionality\_reduction}}}{\emph{\DUrole{n}{word\_embeddings}}, \emph{\DUrole{n}{n\_components}\DUrole{p}{:} \DUrole{n}{int}}, \emph{\DUrole{n}{reduction\_strategy}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}pca\textquotesingle{}}}, \emph{\DUrole{n}{random\_state}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{\_tsne\_perplexity}\DUrole{o}{=}\DUrole{default_value}{5}}, \emph{\DUrole{n}{\_tsne\_learning\_rate}\DUrole{o}{=}\DUrole{default_value}{10}}, \emph{\DUrole{n}{\_tsne\_n\_iter}\DUrole{o}{=}\DUrole{default_value}{3000}}}{}
Apply dimensionality reduction on words embeddings using reduction
techniques like the Principal Component Analysis (PCA) and the T\sphinxhyphen{}distributed
Stochastic Neighbour Embedding (t\sphinxhyphen{}SNE).  The default values for the perplexity,
learning rate and number of iterations have been empirically tuned to
those that produced acceptable results consistently.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{word\_embeddings}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Collection of word embeddings.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{n\_components}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Dimension of the new embedded space (e.g., 2 for
2D visualization, 3 for 3D visualization).

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{random\_state}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Random state for dimensionality reduction techniques,
defaults to None

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{reduction\_strategy}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Reduction strategy to choose. Can be either
‘pca’ or ‘tsne’. Defaults to ‘pca’

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{\_tsne\_perplexity}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The perplexity is related to the number of
nearest neighbors that is used in other manifold learning algorithms.
Defaults to 5.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{\_tsne\_learning\_rate}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The learning rate for t\sphinxhyphen{}SNE is usually
in the range {[}10.0, 1000.0{]}. Defaults to 10.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{\_tsne\_n\_iter}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Maximum number of iterations without progress
to abort optimization process, defaults to 3000.

\end{itemize}

\item[{Raises}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{ValueError}} \textendash{} Raise \sphinxtitleref{ValueError} if \sphinxtitleref{reduction\_stragy} takes
an ilegal value.

\item[{Returns}] \leavevmode
Data in the new embedded space.

\item[{Return type}] \leavevmode
NumPy array

\end{description}\end{quote}

\end{fulllineitems}

\index{display\_embeddings\_scatterplot\_2D() (in module code\_utils.plotter)@\spxentry{display\_embeddings\_scatterplot\_2D()}\spxextra{in module code\_utils.plotter}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.plotter.display_embeddings_scatterplot_2D}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.plotter.}}\sphinxbfcode{\sphinxupquote{display\_embeddings\_scatterplot\_2D}}}{\emph{\DUrole{n}{word\_embeddings}}, \emph{\DUrole{n}{color\_text}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{random\_state}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{reduction\_strategy}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}pca\textquotesingle{}}}, \emph{\DUrole{n}{\_graph\_showlegend}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{tsne\_params}\DUrole{o}{=}\DUrole{default_value}{\{\}}}, \emph{\DUrole{o}{**}\DUrole{n}{scatter\_params}}}{}
Visualize BERT embeddings in a 2D scatterplot using dimensionality
reduction techniques like Principal Component Analysis (PCA) and the T\sphinxhyphen{}distributed
Stochastic Neighbour Embedding (t\sphinxhyphen{}SNE).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{word\_embeddings}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Collection of word embeddings.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{color\_text}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Label for plotly scatterplot \sphinxtitleref{color} attribute.
Defaults to None

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{random\_state}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Random state for dimensionality reduction techniques,
defaults to None

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{reduction\_strategy}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Reduction strategy to choose. Can be either
‘pca’ or ‘tsne’. Defaults to ‘pca’

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{\_graph\_showlegend}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Show legend, defaults to True

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{tsne\_params}} \textendash{} Additional parameters for t\sphinxhyphen{}SNE,
defaults to \{\}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{display\_embeddings\_scatterplot\_3D() (in module code\_utils.plotter)@\spxentry{display\_embeddings\_scatterplot\_3D()}\spxextra{in module code\_utils.plotter}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.plotter.display_embeddings_scatterplot_3D}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.plotter.}}\sphinxbfcode{\sphinxupquote{display\_embeddings\_scatterplot\_3D}}}{\emph{\DUrole{n}{word\_embeddings}}, \emph{\DUrole{n}{color\_text}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{n}{random\_state}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{reduction\_strategy}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}pca\textquotesingle{}}}, \emph{\DUrole{n}{\_graph\_showlegend}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{tsne\_params}\DUrole{o}{=}\DUrole{default_value}{\{\}}}, \emph{\DUrole{o}{**}\DUrole{n}{scatter\_params}}}{}
Visualize BERT embeddings in a 3D scatterplot using dimensionality
reduction techniques like Principal Component Analysis (PCA) and the T\sphinxhyphen{}distributed
Stochastic Neighbour Embedding (t\sphinxhyphen{}SNE).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{word\_embeddings}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Collection of word embeddings.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{color\_text}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Label for plotly scatterplot \sphinxtitleref{color} attribute.
Defaults to None

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{random\_state}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Random state for dimensionality reduction techniques,
defaults to None

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{reduction\_strategy}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Reduction strategy to choose. Can be either
‘pca’ or ‘tsne’. Defaults to ‘pca’

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{\_graph\_showlegend}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Show legend, defaults to True

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{tsne\_params}} \textendash{} Additional parameters for t\sphinxhyphen{}SNE,
defaults to \{\}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{histogram\_embeddings\_nn() (in module code\_utils.plotter)@\spxentry{histogram\_embeddings\_nn()}\spxextra{in module code\_utils.plotter}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.plotter.histogram_embeddings_nn}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.plotter.}}\sphinxbfcode{\sphinxupquote{histogram\_embeddings\_nn}}}{\emph{\DUrole{n}{data}\DUrole{p}{:} \DUrole{n}{list}}}{}
Plot histogram for the nearest neighbors of an embedding.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{data}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Two dimensional array containing a collection of similar
words (first component), list of similarity scores (second component),
and a list of labels (third component).

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_bert\_embeddings\_nn() (in module code\_utils.plotter)@\spxentry{plot\_bert\_embeddings\_nn()}\spxextra{in module code\_utils.plotter}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.plotter.plot_bert_embeddings_nn}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.plotter.}}\sphinxbfcode{\sphinxupquote{plot\_bert\_embeddings\_nn}}}{\emph{\DUrole{n}{model}}, \emph{\DUrole{n}{vocab}\DUrole{p}{:} \DUrole{n}{dict}}, \emph{\DUrole{n}{input\_words}}, \emph{\DUrole{n}{k}\DUrole{o}{=}\DUrole{default_value}{5}}, \emph{\DUrole{n}{display\_option}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}3d\textquotesingle{}}}, \emph{\DUrole{n}{reduction\_strategy}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}pca\textquotesingle{}}}, \emph{\DUrole{n}{random\_state}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
Visualize the \sphinxtitleref{k} most similar words in \sphinxtitleref{vocab} in the 2D or 3D
embedding space.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model}} (\sphinxstyleliteralemphasis{\sphinxupquote{BertModel}}) \textendash{} Bert pretrained model.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{vocab}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} Tokenizer vocabulary.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{input\_words}} (\sphinxstyleliteralemphasis{\sphinxupquote{Iterable}}) \textendash{} Words, the KNN of which are to be calculated and
displayed.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} number of similar words to visualize. By default, 5

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{display\_option}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Visualize BERT embeddings either in ‘2d’ or
‘3d’, defaults to ‘3d’

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{reduction\_strategy}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Reduction strategy to choose. Can be either
‘pca’ or ‘tsne’. Defaults to ‘pca’

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{random\_state}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Random state for dimensionality reduction techniques,
defaults to None

\end{itemize}

\item[{Raises}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{ValueError}} \textendash{} Raise \sphinxtitleref{ValueError} if \sphinxtitleref{display\_option} takes an
ilegal value.

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_heatmap() (in module code\_utils.plotter)@\spxentry{plot\_heatmap()}\spxextra{in module code\_utils.plotter}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.plotter.plot_heatmap}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.plotter.}}\sphinxbfcode{\sphinxupquote{plot\_heatmap}}}{\emph{\DUrole{n}{data}}, \emph{\DUrole{n}{color\_continuous\_scale}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
Plot heatmap. Wrapper of the plotly \sphinxtitleref{imshow} function.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data}} (\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} 2D data to be plotted.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{color\_continuous\_scale}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Colour scale to be used in the heatmap,
defaults to None

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_heatmap\_embeddings() (in module code\_utils.plotter)@\spxentry{plot\_heatmap\_embeddings()}\spxextra{in module code\_utils.plotter}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.plotter.plot_heatmap_embeddings}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.plotter.}}\sphinxbfcode{\sphinxupquote{plot\_heatmap\_embeddings}}}{\emph{\DUrole{n}{model}}, \emph{\DUrole{n}{tokenizer}}, \emph{\DUrole{n}{data}\DUrole{p}{:} \DUrole{n}{pandas.core.frame.DataFrame}}, \emph{\DUrole{n}{polysemous\_word}\DUrole{p}{:} \DUrole{n}{str}}, \emph{\DUrole{n}{color\_continuous\_scale}\DUrole{p}{:} \DUrole{n}{str} \DUrole{o}{=} \DUrole{default_value}{None}}}{}
Plot heatmap of the cosine similarity of all different contextual embeddings
of \sphinxtitleref{polysemous\_word} in \sphinxtitleref{data}. This experiment is explained in  the “Exploring
BERT contextual representations” section of the dissertation. (Disclaimer:
sorry about poor code readability).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model}} (\sphinxstyleliteralemphasis{\sphinxupquote{BertModel}}) \textendash{} Bert pretrained model.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{tokenizer}} (\sphinxstyleliteralemphasis{\sphinxupquote{BertTokenizer}}) \textendash{} Bert precomputed tokenizer.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data}} (\sphinxstyleliteralemphasis{\sphinxupquote{pd.DataFrame}}) \textendash{} Test data for WSD evaluation.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{polysemous\_word}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Polysemous word.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{color\_continuous\_scale}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Colour scale to be used in the heatmap,
defaults to None

\end{itemize}

\item[{Returns}] \leavevmode
The collection of the different contextual embeddings, along
with the labels.

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_scatter\_with\_secondary\_y\_axis() (in module code\_utils.plotter)@\spxentry{plot\_scatter\_with\_secondary\_y\_axis()}\spxextra{in module code\_utils.plotter}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.plotter.plot_scatter_with_secondary_y_axis}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.plotter.}}\sphinxbfcode{\sphinxupquote{plot\_scatter\_with\_secondary\_y\_axis}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{y}}, \emph{\DUrole{n}{y2}}, \emph{\DUrole{n}{fig\_title}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}\textquotesingle{}}}, \emph{\DUrole{n}{x\_title}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}\textquotesingle{}}}, \emph{\DUrole{n}{y\_title}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}\textquotesingle{}}}, \emph{\DUrole{n}{y2\_title}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}\textquotesingle{}}}}{}
Plot scatter plot with secondary y axis.

\end{fulllineitems}

\index{write\_embeddings\_to\_disk() (in module code\_utils.plotter)@\spxentry{write\_embeddings\_to\_disk()}\spxextra{in module code\_utils.plotter}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:code_utils.plotter.write_embeddings_to_disk}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{code\_utils.plotter.}}\sphinxbfcode{\sphinxupquote{write\_embeddings\_to\_disk}}}{\emph{\DUrole{n}{model}}, \emph{\DUrole{n}{vocab}\DUrole{p}{:} \DUrole{n}{dict}}, \emph{\DUrole{n}{out\_dir}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}runs/bert\_embeddings\textquotesingle{}}}, \emph{\DUrole{n}{write\_word\_embeddings}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{write\_position\_embeddings}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{write\_type\_embeddings}\DUrole{o}{=}\DUrole{default_value}{False}}}{}~\begin{description}
\item[{Utility to write embeddings weights to disk that can be loaded with}] \leavevmode
TensorBoard to visualize the embeddings.

\end{description}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model}} (\sphinxstyleliteralemphasis{\sphinxupquote{BertModel}}) \textendash{} Bert pretrained model.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{vocab}} (\sphinxstyleliteralemphasis{\sphinxupquote{dict}}) \textendash{} Tokenizer vocabulary.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{out\_dir}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Directory to write the embeddings, defaults to 
‘runs/bert\_embeddings’.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{write\_word\_embeddings}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Write word embeddings to disk, defaults
to True.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{write\_position\_embeddings}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Write position embeddings to disk,
defaults to False.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{write\_type\_embeddings}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Write token type embeddings to disk,
defaults to False.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}



\chapter{Code used for experiments.}
\label{\detokenize{code:code-used-for-experiments}}
Documentation for the experiments described in the dissertation.


\section{Speedup\sphinxhyphen{}recall tradeoff depending on the number of trees used in ANNOY(\sphinxstyleliteralintitle{\sphinxupquote{experiment\_annoy\_ntrees.py}})}
\label{\detokenize{code:module-experiment_annoy_ntrees}}\label{\detokenize{code:speedup-recall-tradeoff-depending-on-the-number-of-trees-used-in-annoy-experiment-annoy-ntrees-py}}\index{module@\spxentry{module}!experiment\_annoy\_ntrees@\spxentry{experiment\_annoy\_ntrees}}\index{experiment\_annoy\_ntrees@\spxentry{experiment\_annoy\_ntrees}!module@\spxentry{module}}
Experiment to test peedup\sphinxhyphen{}recall tradeoff depending on the number of trees 
used in ANNOY.

Author: David Lorenzo Alfaro.
\index{evaluate\_n\_trees() (in module experiment\_annoy\_ntrees)@\spxentry{evaluate\_n\_trees()}\spxextra{in module experiment\_annoy\_ntrees}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:experiment_annoy_ntrees.evaluate_n_trees}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{experiment\_annoy\_ntrees.}}\sphinxbfcode{\sphinxupquote{evaluate\_n\_trees}}}{\emph{\DUrole{n}{queries}\DUrole{p}{:} \DUrole{n}{list}}, \emph{\DUrole{n}{search\_alg}}, \emph{\DUrole{n}{k}\DUrole{o}{=}\DUrole{default_value}{5}}, \emph{\DUrole{n}{verbose}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
Utility used to evaluate the speedup\sphinxhyphen{}recall tradeoff of ANNOY as
the number of trees increases.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{queries}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Collection of textual queries.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{search\_alg}} ({\hyperref[\detokenize{code:semantic_search.SemanticSearch}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{SemanticSearch}}}}}) \textendash{} Semantic search object.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{k}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Top k elements to consider in the comparison, defaults to 5

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{verbose}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Verbosity mode, defaults to True

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}



\section{Evaluate text summarization using different values of \sphinxstyleemphasis{k} top sentences (\sphinxstyleliteralintitle{\sphinxupquote{experiment\_text\_summarization.py}})}
\label{\detokenize{code:module-experiment_text_summarization}}\label{\detokenize{code:evaluate-text-summarization-using-different-values-of-k-top-sentences-experiment-text-summarization-py}}\index{module@\spxentry{module}!experiment\_text\_summarization@\spxentry{experiment\_text\_summarization}}\index{experiment\_text\_summarization@\spxentry{experiment\_text\_summarization}!module@\spxentry{module}}
Experiment to evaluate text summarization using different values of \sphinxstyleemphasis{k} top
sentences.

Author: David Lorenzo Alfaro.
\index{evaluate\_summarization\_candidates() (in module experiment\_text\_summarization)@\spxentry{evaluate\_summarization\_candidates()}\spxextra{in module experiment\_text\_summarization}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{code:experiment_text_summarization.evaluate_summarization_candidates}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{experiment\_text\_summarization.}}\sphinxbfcode{\sphinxupquote{evaluate\_summarization\_candidates}}}{\emph{\DUrole{n}{corpus\_overviews}\DUrole{p}{:} \DUrole{n}{list}}, \emph{\DUrole{n}{candidates}}, \emph{\DUrole{n}{embedder}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{spacy\_model}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{tokenizer}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
Get an array of reduction rates and number of word pieces for a set of
candidate number of sentences used to summarize each book overview (must 
be a list or array\sphinxhyphen{}like of integers).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{corpus\_overviews}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{array\sphinxhyphen{}like}}) \textendash{} Book overviews.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{candidates}} (\sphinxstyleliteralemphasis{\sphinxupquote{Iterable}}) \textendash{} List of number of candidates to evaluate.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{embedder}} (\sphinxstyleliteralemphasis{\sphinxupquote{SentenceTransformer}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instance of \sphinxtitleref{SentenceTransformer}. If \sphinxtitleref{None}, it loads the default
pretrained sentence transformer model. Defaults to None.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{spacy\_model}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Name of a spacy pretrained tokenizer model. If \sphinxtitleref{None}, it
loads the default model. Defaults to None.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{tokenizer}} (\sphinxstyleliteralemphasis{\sphinxupquote{BertTokenizer}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instance of \sphinxtitleref{BertTokenizer} class. If \sphinxtitleref{None}, it loads the
predefined tokenizer of ‘bert\sphinxhyphen{}base\sphinxhyphen{}uncased’.

\end{itemize}

\item[{Returns}] \leavevmode
Summarized overviews with at most \sphinxtitleref{candidates} sentences.

\item[{Return type}] \leavevmode
list

\end{description}\end{quote}

\end{fulllineitems}



\chapter{Visualization of BERT embeddings.}
\label{\detokenize{Visualization of BERT embeddings:Visualization-of-BERT-embeddings.}}\label{\detokenize{Visualization of BERT embeddings::doc}}


As part of the undergraduate dissertation: Similarity Measures in Natural Language Processing based on Deep Learning Models.





David Lorenzo Alfaro




\section{Introduction.}
\label{\detokenize{Visualization of BERT embeddings:Introduction.}}
BERT embeddings have a dimensionality of 768. As studied, linguistic features are be encoded along the representation of the word in a distributed fashion. Whilst embeddings work particularly well for computer systems to handle semantic and syntactic information about natural language, if humans were to visualize sequences of 768 floating point values, observing any valuable information would be an extremely difficult task. Furthermore, when handling natural language, the most common scenario
involves dealing with sequences of words, thus posing neural NLP as a high\sphinxhyphen{}dimensionality problem. Consequently, the great majority of visualization techniques for embeddings involves dimensionality reduction.

The most common dimensionality reduction techniques are the Principal Component Analysis (PCA) and the T\sphinxhyphen{}distributed Stochastic Neighbour Embedding (t\sphinxhyphen{}SNE). The former is usually preferred because it can be used as a black box. That is, since it is a non\sphinxhyphen{}parametric method, it is not usually required to know the mathematics behind PCA.

The following sections provide a general overview of the different experiments carried out to visualize BERT embeddings. To that end, the increasingly popular open source graphing library Plotly (\sphinxurl{https://plotly.com/}) is used. Plotly is a simple but expressive capable library that enables creating interactive charts and maps, a pretty much desired characteristic for the visualization tasks hereinafter described.


\section{Visualization of kNN neighbours of BERT’s pretrained embeddings.}
\label{\detokenize{Visualization of BERT embeddings:Visualization-of-kNN-neighbours-of-BERT_u2019s-pretrained-embeddings.}}
The first test consist in gathering the WordPiece token embeddings weights learned through training of a BERT model for some query words to then find the k nearest neighbours to each word in the query. Let us first load all necessary libraries and modules.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[1]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{code\PYGZus{}utils}\PYG{n+nn}{.}\PYG{n+nn}{paths} \PYG{k+kn}{import} \PYG{o}{*}
\PYG{c+c1}{\PYGZsh{} Plotter module contain a bunch of utilities for}
\PYG{c+c1}{\PYGZsh{} BERT embeddings visualization.}
\PYG{k+kn}{from} \PYG{n+nn}{code\PYGZus{}utils} \PYG{k+kn}{import} \PYG{n}{utils}\PYG{p}{,} \PYG{n}{plotter}

\end{sphinxVerbatim}
}

For the sake of simplicity, we decided to use BERT’s base uncased pretrained model. This model has way less parameters and weights, hence is lighter. We have not run this experiment on any pretrained model based on BERT Large architecture, albeit differences in outcomes, if any, must be negligible.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[2]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{pretrained\PYGZus{}model} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bert\PYGZhy{}base\PYGZhy{}uncased}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{c+c1}{\PYGZsh{} Get BERT pretained model.}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{utils}\PYG{o}{.}\PYG{n}{get\PYGZus{}bert\PYGZus{}model}\PYG{p}{(}\PYG{n}{pretrained\PYGZus{}model}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Get BERT precomputed tokenizer.}
\PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{utils}\PYG{o}{.}\PYG{n}{get\PYGZus{}bert\PYGZus{}tokenizer}\PYG{p}{(}\PYG{n}{pretrained\PYGZus{}model}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Get tokenizer vocabulary.}
\PYG{n}{vocab} \PYG{o}{=} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{vocab}

\PYG{c+c1}{\PYGZsh{} To guarantee experiment reproducibility}
\PYG{n}{random\PYGZus{}state} \PYG{o}{=} \PYG{l+m+mi}{0}
\end{sphinxVerbatim}
}

It is worth mentioning that the way in which experiment has been run is not appropriate for BERT for several reasons. First and foremost, BERT derives context\sphinxhyphen{}sensitive subword representations. For BERT to actually derive the proper embedding for a specific token, some context (i.e., a sentence containing that word) must be provided. In the approach we are following, comparisons among representations make no use of context, which would just be fine for context\sphinxhyphen{}independent embeddings such as
those derived by techniques like GloVe or Word2vec. Second, since no actual embeddings are being computed (i.e., the learned weights for each token in BERT’s vocab are used), all query words do necessarily need to belong to the BERT’s model vocab, hence noticeably constraining the range of possible words to use in the query. Under these assumptions, we consider this experiment, albeit not entirely correct, can help the reader to have a better understanding of some of the concepts previously
discussed, especially those concerning the linguistic regularities in continuous space word representations.


\bigskip\hrule\bigskip


The process is straightforward. Given a subset of BERT’s vocabulary tokens, get the learned embeddings and compute the k nearest neighbours to each query word. The set of neighbour candidates is the complete vocabulary used in BERT (30,000 tokens). Since it is a sine qua non condition that the query tokens belong to BERT’s vocab, the nearest neighbour for each query token is necessarily the token itself. We found that the easiest and computationally cheapest way to prevent this from happening is
to get the \(k+1\) nearest neighbours to then discard those that are incorrect.

Afterwards, the proper embeddings for the kNN of all query tokens are fed into one dimensionality reduction technique. Since it is our will that the output of either PCA or t\sphinxhyphen{}SNE are visually interpretable, the embeddings, originally in the 768\sphinxhyphen{}dimensional space, are compressed into either 2\sphinxhyphen{}dimensional or 3\sphinxhyphen{}dimensional representations, attempting to preserve some of the meaningful information.

For further details, check the documentation for \sphinxcode{\sphinxupquote{plot\_bert\_embeddings\_nn}} method.

In the dissertation we discussed that Word2vec embeddings were able to capture features like the notion of gender or the syntactic singular/plural relation. For this first query, we will be using again the word queen.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[3]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{query} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{queen}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{c+c1}{\PYGZsh{} Format query words.}
\PYG{n}{input\PYGZus{}words} \PYG{o}{=} \PYG{n}{query}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{lower}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{k}\PYG{o}{=}\PYG{l+m+mi}{8}

\PYG{c+c1}{\PYGZsh{} Invoke method to plot PCA 2D projection of the embeddings}
\PYG{c+c1}{\PYGZsh{} and a histogram for all nearest neighbours of the query}
\PYG{c+c1}{\PYGZsh{} word embeddings.}
\PYG{n}{plotter}\PYG{o}{.}\PYG{n}{plot\PYGZus{}bert\PYGZus{}embeddings\PYGZus{}nn}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,}
                                \PYG{n}{vocab}\PYG{p}{,}
                                \PYG{n}{input\PYGZus{}words}\PYG{p}{,}
                                \PYG{n}{k}\PYG{o}{=}\PYG{n}{k}\PYG{p}{,}
                                \PYG{n}{reduction\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pca}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                \PYG{n}{display\PYGZus{}option}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{n}{random\PYGZus{}state}\PYG{p}{)}
\end{sphinxVerbatim}
}

\hrule height -\fboxrule\relax
\vspace{\nbsphinxcodecellspacing}

\makeatletter\setbox\nbsphinxpromptbox\box\voidb@x\makeatother

\begin{nbsphinxfancyoutput}



\end{nbsphinxfancyoutput}

\hrule height -\fboxrule\relax
\vspace{\nbsphinxcodecellspacing}

\makeatletter\setbox\nbsphinxpromptbox\box\voidb@x\makeatother

\begin{nbsphinxfancyoutput}

\begin{sphinxadmonition}{warning}{}\unskip
Data type cannot be displayed: application/vnd.plotly.v1+json, text/html
\end{sphinxadmonition}

\end{nbsphinxfancyoutput}

\hrule height -\fboxrule\relax
\vspace{\nbsphinxcodecellspacing}

\makeatletter\setbox\nbsphinxpromptbox\box\voidb@x\makeatother

\begin{nbsphinxfancyoutput}

\begin{sphinxadmonition}{warning}{}\unskip
Data type cannot be displayed: application/vnd.plotly.v1+json, text/html
\end{sphinxadmonition}

\end{nbsphinxfancyoutput}

As it can be seen, the eight closest words to queen are, in order, king, queens, princess, empress, prince, duchess, countess and monarch. The results are astoundingly positive: all neighbours are reasonably related, either syntactically (e.g., queens) or semantically (e.g., king). Furthermore, the similarity scores obtained denote strong relativeness between the similar and query words.

It is also worth noting that, excepting king and prince (which, on their own are fairly related to queen), all remaining nearest neighbours are royal titles held by women, which hints BERT embeddings capability to capture the notion of gender. Moreover, the words empress, countess, and duchess seem to be closer in space, probably due to syntactic similarities among them (i.e., they all are suffixed with “ess”).


\bigskip\hrule\bigskip


Let us now repeat the experiment using a set of queries

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[4]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{queries} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{stupid, queen, wizard, spain, brother}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{c+c1}{\PYGZsh{} Format query words.}
\PYG{n}{input\PYGZus{}words} \PYG{o}{=} \PYG{n}{queries}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{lower}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{k}\PYG{o}{=}\PYG{l+m+mi}{5}

\PYG{c+c1}{\PYGZsh{} Invoke method to plot T\PYGZhy{}SN 2D projection of the embeddings}
\PYG{c+c1}{\PYGZsh{} and a histogram for all nearest neighbours of the query}
\PYG{c+c1}{\PYGZsh{} word embeddings.}
\PYG{n}{plotter}\PYG{o}{.}\PYG{n}{plot\PYGZus{}bert\PYGZus{}embeddings\PYGZus{}nn}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,}
                                \PYG{n}{vocab}\PYG{p}{,}
                                \PYG{n}{input\PYGZus{}words}\PYG{p}{,}
                                \PYG{n}{k}\PYG{o}{=}\PYG{n}{k}\PYG{p}{,}
                                \PYG{n}{reduction\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t\PYGZhy{}sne}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                \PYG{n}{display\PYGZus{}option}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{n}{random\PYGZus{}state}\PYG{p}{)}
\end{sphinxVerbatim}
}

\hrule height -\fboxrule\relax
\vspace{\nbsphinxcodecellspacing}

\makeatletter\setbox\nbsphinxpromptbox\box\voidb@x\makeatother

\begin{nbsphinxfancyoutput}

\begin{sphinxadmonition}{warning}{}\unskip
Data type cannot be displayed: application/vnd.plotly.v1+json, text/html
\end{sphinxadmonition}

\end{nbsphinxfancyoutput}

\hrule height -\fboxrule\relax
\vspace{\nbsphinxcodecellspacing}

\makeatletter\setbox\nbsphinxpromptbox\box\voidb@x\makeatother

\begin{nbsphinxfancyoutput}

\begin{sphinxadmonition}{warning}{}\unskip
Data type cannot be displayed: application/vnd.plotly.v1+json, text/html
\end{sphinxadmonition}

\end{nbsphinxfancyoutput}

As hinted by the results, each query word, along with its most similar words in terms of cosine\sphinxhyphen{}similarity, conform a cluster. Unless query words are highly correlated, this is to expect, being as cosine\sphinxhyphen{}similarity is a measurement of distance in the d\sphinxhyphen{}dimensional space. Consistently with the highlighted observations for the previous experiment, all nearest neighbours to the query words are intimately connected syntactically and/or semantically, with the cosine similarity scores indicating
strong relatedness.

Generally, some of the words populating the nearest neighbour set of each query word are synonyms (e.g., wizard and magician, stupid and dumb). Also, for the query word Spain, some of the most similar words are countries as well (e.g., Portugal, Italy). BERT embeddings showcase, once more, its capability to learn fine\sphinxhyphen{}grained features about natural language. For the query word brother some of the similar words are terms that belong to the lineal kinship system used in the English\sphinxhyphen{}speaking world
(e.g., son, father).

It is also worth noting that, according to empirical observations, t\sphinxhyphen{}SNE dimensionality reduction technique is encouraged over PCA as the number of query words increase, being as it has reportedly produced better defined clusters, which favours the proper visualization of the results. Unlike PCA, t\sphinxhyphen{}SNE is a parametric method. The default values for the perplexity, learning rate and number of iterations have been empirically tuned to those that produced acceptable results consistently.


\subsection{Visualizing BERT embeddings with Tensorboard.}
\label{\detokenize{Visualization of BERT embeddings:Visualizing-BERT-embeddings-with-Tensorboard.}}
Yet another manner to visualize BERT is using TensorBoard , the TensorFlow’s open source visualization toolkit which provides all the necessary logic to project the embeddings to a lower dimensional space and to make queries in real time. To that end, we have to write the word embeddings learned weights into disk using a utility described in the \sphinxcode{\sphinxupquote{plotter}} python module.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[5]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{c+c1}{\PYGZsh{} Invoking a method that writes embeddings weights to disk that}
\PYG{c+c1}{\PYGZsh{} can be loaded with TensorBoard to visualize the embeddings.}
\PYG{n}{out\PYGZus{}dir} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{runs/bert\PYGZus{}embeddings}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{plotter}\PYG{o}{.}\PYG{n}{write\PYGZus{}embeddings\PYGZus{}to\PYGZus{}disk}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,}
                                 \PYG{n}{vocab}\PYG{p}{,}
                                 \PYG{n}{out\PYGZus{}dir}\PYG{o}{=}\PYG{n}{out\PYGZus{}dir}\PYG{p}{,}
                                 \PYG{n}{write\PYGZus{}word\PYGZus{}embeddings}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
                                 \PYG{n}{write\PYGZus{}position\PYGZus{}embeddings}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}
                                 \PYG{n}{write\PYGZus{}type\PYGZus{}embeddings}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}
}

You can then run TensorBoard. For example, for \sphinxcode{\sphinxupquote{out\_dir = \textquotesingle{}runs/bert\_embeddings}}, you would need to input the following command on a Python console:

\sphinxcode{\sphinxupquote{tensorboard \sphinxhyphen{}\sphinxhyphen{}logdir="\textless{}current notebook folder path\textgreater{}\textbackslash{}runs"}}


\bigskip\hrule\bigskip



\section{Exploring BERT contextual representations.}
\label{\detokenize{Visualization of BERT embeddings:Exploring-BERT-contextual-representations.}}
As previously studied, the power of BERT lies in its ability to derive representations based on context. In this experiment, the embeddings of a word in different contexts (i.e., in different sentences) were computed to check whether there are significant differences among them.

To that end, we used a public domain licensed dataset for word sense disambiguation (WSD) available in \sphinxhref{https://www.kaggle.com/udayarajdhungana/test-data-for-word-sense-disambiguation}{Kaggle}. The dataset file is an excel file with three columns. The first is the serial number (SN), which assigns a unique identifier to each tuple of contextual sentence (second column) and target polysemous word contained in the sentence (third column).

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[6]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{PATH\PYGZus{}WSD\PYGZus{}DATASET} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{visualization/test data for WSD evaluation \PYGZus{}2905.xlsx}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{c+c1}{\PYGZsh{} Load WSD dataset.}
\PYG{n}{df\PYGZus{}polysemy} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}excel}\PYG{p}{(}\PYG{n}{PATH\PYGZus{}WSD\PYGZus{}DATASET}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Set the serial number column as the index column.}
\PYG{n}{df\PYGZus{}polysemy} \PYG{o}{=} \PYG{n}{df\PYGZus{}polysemy}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{n}{df\PYGZus{}polysemy}\PYG{o}{.}\PYG{n}{sn}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Let us sample ten random instances.}
\PYG{n}{df\PYGZus{}polysemy}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{n}{random\PYGZus{}state}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[6]:\,\hspace{\fboxrule}\hspace{\fboxsep}}        sn                                   sentence/context polysemy\_word
sn
583    583                               I lost my AC remote.        remote
1812  1812                             a young man is running           man
2250  2250                       Crane is a large water bird.         crane
1653  1653                    Insert the jack in the LAN port          jack
668    668                   Thank you for your prompt reply.        prompt
515    515                        She likes Russian pop song.           pop
619    619  Films are rated on a scale of poor, fair, good{\ldots}         scale
2692  2692               The pilot is checking belly of plane         belly
2491  2491  We want to appeal to our core supporters witho{\ldots}          core
937    937  Any type of single cut metal file can be used {\ldots}          file
\end{sphinxVerbatim}
}

Let us now use one of the many polysemous words in the dataset.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[7]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{polysemous\PYGZus{}word}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bank}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{n}{df\PYGZus{}polysemy}\PYG{p}{[}\PYG{n}{df\PYGZus{}polysemy}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{polysemy\PYGZus{}word}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{==} \PYG{n}{polysemous\PYGZus{}word}\PYG{p}{]}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[7]:\,\hspace{\fboxrule}\hspace{\fboxsep}}    sn                                   sentence/context polysemy\_word
sn
1    1                               I have bank account.          bank
2    2               Loan amount is approved by the bank.          bank
3    3  He returned to office after he deposited cash {\ldots}          bank
4    4     They started using new software in their bank.          bank
5    5                   he went to bank balance inquiry.          bank
6    6  I wonder why some bank have more interest rate{\ldots}          bank
7    7  You have to deposit certain percentage of your{\ldots}          bank
8    8                          He took loan from a Bank.          bank
9    9                 he is waking along the river bank.          bank
10  10          The red boat in the bank is already sold.          bank
11  11  Spending time on the bank of Kaligandaki river{\ldots}          bank
12  12         He was sitting on sea bank with his friend          bank
13  13  She has always dreamed of spending a vacation {\ldots}          bank
14  14   Bank of a river is very pleasant place to enjoy.          bank
\end{sphinxVerbatim}
}

As it can be seen, the dataset contains fourteen different sentences with the target word bank. Sentences from 1 to 8 refer to bank as as \sphinxstyleemphasis{“an organization where people and businesses can invest or borrow money, change it to foreign money, etc., or a building where these services are offered”}; whereas sentences from 9 to 14 refer to bank as a \sphinxstyleemphasis{“sloping raised land, especially along the sides of a river”} (definitions from Cambridge Dictionary). It would be desirable for BERT to be able to
disambiguate both senses of the word, which would translate to being capable of deriving distant representations in the n\sphinxhyphen{}dimensional space for the different meanings of bank. As we have studied, the self\sphinxhyphen{}attention mechanism in BERT’s model architecture is responsible for baking into the representation of each word in a sequence salient information about the rest of the words in the sequence.

This outstanding characteristic of the created representations by BERT exhibit an intrinsic degree of natural language understanding never seen any time before (even at pretraining!). For instance, the context words in the first three sentences of the previous table are different and refer to distinct concepts (e.g., account, loan, amount, deposited, cash). That notwithstanding, they are all related because they all belong to a semantic field of interconnected words that can be used in similar
contexts.

Let us test whether BERT realizes this connection and embeds it into the resultant representations.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[8]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{context\PYGZus{}embeddings}\PYG{p}{,} \PYG{n}{labels} \PYG{o}{=} \PYG{n}{plotter}\PYG{o}{.}\PYG{n}{plot\PYGZus{}heatmap\PYGZus{}embeddings}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,}
                                                             \PYG{n}{tokenizer}\PYG{p}{,}
                                                             \PYG{n}{df\PYGZus{}polysemy}\PYG{p}{,}
                                                             \PYG{n}{polysemous\PYGZus{}word}\PYG{p}{)}
\end{sphinxVerbatim}
}

\hrule height -\fboxrule\relax
\vspace{\nbsphinxcodecellspacing}

\makeatletter\setbox\nbsphinxpromptbox\box\voidb@x\makeatother

\begin{nbsphinxfancyoutput}

\begin{sphinxadmonition}{warning}{}\unskip
Data type cannot be displayed: application/vnd.plotly.v1+json, text/html
\end{sphinxadmonition}

\end{nbsphinxfancyoutput}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Context sentence for each contextual embedding of 'bank'.

bank\_1: I have bank account.
bank\_2: Loan amount is approved by the bank.
bank\_3: He returned to office after he deposited cash in the bank.
bank\_4: They started using new software in their bank.
bank\_5: he went to bank balance inquiry.
bank\_6: I wonder why some bank have more interest rate than others.
bank\_7: You have to deposit certain percentage of your salary in the bank.
bank\_8: He took loan from a Bank.
bank\_9: he is waking along the river bank.
bank\_10: The red boat in the bank is already sold.
bank\_11: Spending time on the bank of Kaligandaki river was his way of enjoying in his childhood.
bank\_12: He was sitting on sea bank with his friend
bank\_13: She has always dreamed of spending a vacation on a bank of Caribbean sea.
bank\_14: Bank of a river is very pleasant place to enjoy.
\end{sphinxVerbatim}
}

As it can be seen, BERT seems to, in fact, realize of this connection. Therefore, the embeddings for the target word when referred as a financial institution exhibit strong similarity (the contextual baked into the embedding is similar). Analogously, the embeddings for bank in sentences from 9 to 14 are very similar, and they are all farther from those of sentences from 1 to 8, excepting that of the tenth sentence (arguably because the word sold is in the sentence).

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[9]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{plotter}\PYG{o}{.}\PYG{n}{display\PYGZus{}embeddings\PYGZus{}scatterplot\PYGZus{}3D}\PYG{p}{(}\PYG{n}{context\PYGZus{}embeddings}\PYG{p}{,}
                                          \PYG{n}{reduction\PYGZus{}strategy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tsne}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                          \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{n}{random\PYGZus{}state}\PYG{p}{,}
                                          \PYG{n}{\PYGZus{}graph\PYGZus{}showlegend}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}
                                          \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Projection for different contextual embeddings}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                          \PYG{n}{text}\PYG{o}{=}\PYG{n}{labels}\PYG{p}{,}
                                          \PYG{n}{color}\PYG{o}{=}\PYG{n}{labels}\PYG{p}{)}
\end{sphinxVerbatim}
}

\hrule height -\fboxrule\relax
\vspace{\nbsphinxcodecellspacing}

\makeatletter\setbox\nbsphinxpromptbox\box\voidb@x\makeatother

\begin{nbsphinxfancyoutput}

\begin{sphinxadmonition}{warning}{}\unskip
Data type cannot be displayed: application/vnd.plotly.v1+json, text/html
\end{sphinxadmonition}

\end{nbsphinxfancyoutput}




\chapter{Exploratory data analysis and data preprocessing.}
\label{\detokenize{Data preprocessing:Exploratory-data-analysis-and-data-preprocessing.}}\label{\detokenize{Data preprocessing::doc}}


As part of the undergraduate dissertation: Similarity Measures in Natural Language Processing based on Deep Learning Models.





David Lorenzo Alfaro




\section{Introduction.}
\label{\detokenize{Data preprocessing:Introduction.}}
To implement a semantic similarity search information retrieval system, a collection of resources is required. We can, then, use similarity measures between searches and data, willing to retrieve relevant information in an efficient fashion.

In our work, we will be using information about ten thousand books from the \sphinxhref{https://www.kaggle.com/zygmunt/goodbooks-10k?select=books.csv}{goodbooks\sphinxhyphen{}10k} \sphinxhref{https://www.kaggle.com}{kaggle} dataset. All the information was retrieved from \sphinxhref{https://www.goodreads.com/}{Goodreads}, the world’s largest site for readers and book recommendations.



The original dataset has been previously modified to better manage the different identifiers and indexes available for each book.



Fortunately, the great majority of word and sentence embedding techniques have been trained on large corpora, often involving humongous number of books (e.g., Toronto Book Corpus). This results on representations that, out of the box, offer great performance for a wide variety of downstream tasks with little fine\sphinxhyphen{}tuning being required.


\section{Dataset exploration.}
\label{\detokenize{Data preprocessing:Dataset-exploration.}}
Before getting started, it is first necessary to load all libraries and dependencies that will be used later in the notebook.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[4]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{code\PYGZus{}utils} \PYG{k+kn}{import} \PYG{n}{utils}
\PYG{k+kn}{from} \PYG{n+nn}{code\PYGZus{}utils}\PYG{n+nn}{.}\PYG{n+nn}{paths} \PYG{k+kn}{import} \PYG{o}{*}
\PYG{k+kn}{from} \PYG{n+nn}{pathlib} \PYG{k+kn}{import} \PYG{n}{Path}
\end{sphinxVerbatim}
}

Besides, we set a seed to guarantee reproducibility of the experiments.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[5]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{seed} \PYG{o}{=} \PYG{l+m+mi}{0}
\end{sphinxVerbatim}
}

Let us now unzip containing the collection of books.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[6]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{from} \PYG{n+nn}{shutil} \PYG{k+kn}{import} \PYG{n}{unpack\PYGZus{}archive}
\PYG{n}{unpack\PYGZus{}archive}\PYG{p}{(}\PYG{n}{PATH\PYGZus{}DATASET\PYGZus{}ZIP}\PYG{p}{,} \PYG{n}{DIR\PYGZus{}DATASET}\PYG{p}{)}
\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[7]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{filepath} \PYG{o}{=} \PYG{n}{PATH\PYGZus{}BOOKS}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{read\PYGZus{}csv}\PYG{p}{(}\PYG{n}{filepath}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{  }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
}

To sample the first \sphinxstyleemphasis{n} instances of a dataset we can use the \sphinxcode{\sphinxupquote{head}} function.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[8]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{data}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[8]:\,\hspace{\fboxrule}\hspace{\fboxsep}}   book\_id  gr\_book\_id  gr\_best\_book\_id  work\_id  books\_count       isbn  \textbackslash{}
0        0     2767052          2767052  2792775          272  439023483
1        1           3                3  4640799          491  439554934
2        2       41865            41865  3212258          226  316015849
3        3        2657             2657  3275794          487   61120081
4        4        4671             4671   245494         1356  743273567

         isbn13                      authors  original\_publication\_year  \textbackslash{}
0  9.780439e+12              Suzanne Collins                     2008.0
1  9.780440e+12  J.K. Rowling, Mary GrandPré                     1997.0
2  9.780316e+12              Stephenie Meyer                     2005.0
3  9.780061e+12                   Harper Lee                     1960.0
4  9.780743e+12          F. Scott Fitzgerald                     1925.0

                             original\_title  {\ldots} ratings\_count  \textbackslash{}
0                          The Hunger Games  {\ldots}       4780653
1  Harry Potter and the Philosopher's Stone  {\ldots}       4602479
2                                  Twilight  {\ldots}       3866839
3                     To Kill a Mockingbird  {\ldots}       3198671
4                          The Great Gatsby  {\ldots}       2683664

  work\_ratings\_count  work\_text\_reviews\_count  ratings\_1  ratings\_2  \textbackslash{}
0            4942365                   155254      66715     127936
1            4800065                    75867      75504     101676
2            3916824                    95009     456191     436802
3            3340896                    72586      60427     117415
4            2773745                    51992      86236     197621

   ratings\_3  ratings\_4  ratings\_5  \textbackslash{}
0     560092    1481305    2706317
1     455024    1156318    3011543
2     793319     875073    1355439
3     446835    1001952    1714267
4     606158     936012     947718

                                           image\_url  \textbackslash{}
0  https://images.gr-assets.com/books/1447303603m{\ldots}
1  https://images.gr-assets.com/books/1474154022m{\ldots}
2  https://images.gr-assets.com/books/1361039443m{\ldots}
3  https://images.gr-assets.com/books/1361975680m{\ldots}
4  https://images.gr-assets.com/books/1490528560m{\ldots}

                                     small\_image\_url
0  https://images.gr-assets.com/books/1447303603s{\ldots}
1  https://images.gr-assets.com/books/1474154022s{\ldots}
2  https://images.gr-assets.com/books/1361039443s{\ldots}
3  https://images.gr-assets.com/books/1361975680s{\ldots}
4  https://images.gr-assets.com/books/1490528560s{\ldots}

[5 rows x 23 columns]
\end{sphinxVerbatim}
}

Alternatively, we can use the \sphinxcode{\sphinxupquote{sample}} function, which samples \sphinxstyleemphasis{n} random instances of the dataset.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[9]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{data}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{n}{seed}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[9]:\,\hspace{\fboxrule}\hspace{\fboxsep}}      book\_id  gr\_book\_id  gr\_best\_book\_id  work\_id  books\_count        isbn  \textbackslash{}
9394     9394       38703            38703   575142           43   385733143
898       898       53835            53835  1959512          836  159308143X
2398     2398       43893            43893  1443364           47   765344300
5906     5906       31244            31244  2888469          378   375761144
2343     2343      497199           497199  1132770           80   876852630

            isbn13                        authors  original\_publication\_year  \textbackslash{}
9394  9.780386e+12                   Louis Sachar                     2006.0
898   9.781593e+12  Edith Wharton, Maureen Howard                     1920.0
2398  9.780765e+12                 Terry Goodkind                     2003.0
5906  9.780376e+12                Charles Dickens                     1865.0
2343  9.780877e+12               Charles Bukowski                     1975.0

                         original\_title  {\ldots} ratings\_count work\_ratings\_count  \textbackslash{}
9394                        Small Steps  {\ldots}         11837              13095
898                The Age of Innocence  {\ldots}        102646             114994
2398  Naked Empire (Sword of Truth, \#8)  {\ldots}         39682              42066
5906                  Our Mutual Friend  {\ldots}         18599              20659
2343                           Factotum  {\ldots}         37376              40444

      work\_text\_reviews\_count  ratings\_1  ratings\_2  ratings\_3  ratings\_4  \textbackslash{}
9394                     1387        267       1177       4066       4471
898                      5051       2359       6549      25631      42542
2398                      548       1519       3639       9953      12891
5906                     1102        434        986       3803       6936
2343                     1213        457       1875       8979      16585

      ratings\_5                                          image\_url  \textbackslash{}
9394       3114  https://s.gr-assets.com/assets/nophoto/book/11{\ldots}
898       37913  https://s.gr-assets.com/assets/nophoto/book/11{\ldots}
2398      14064  https://s.gr-assets.com/assets/nophoto/book/11{\ldots}
5906       8500  https://images.gr-assets.com/books/1403189244m{\ldots}
2343      12548  https://images.gr-assets.com/books/1407706616m{\ldots}

                                        small\_image\_url
9394  https://s.gr-assets.com/assets/nophoto/book/50{\ldots}
898   https://s.gr-assets.com/assets/nophoto/book/50{\ldots}
2398  https://s.gr-assets.com/assets/nophoto/book/50{\ldots}
5906  https://images.gr-assets.com/books/1403189244s{\ldots}
2343  https://images.gr-assets.com/books/1407706616s{\ldots}

[5 rows x 23 columns]
\end{sphinxVerbatim}
}

As it can be observed, the dataset has 23 different features. However, only the first and last 10 features of the dataset are being displayed. Let us print the names of all the features in the dataset.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[10]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{data}\PYG{o}{.}\PYG{n}{columns}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[10]:\,\hspace{\fboxrule}\hspace{\fboxsep}}Index(['book\_id', 'gr\_book\_id', 'gr\_best\_book\_id', 'work\_id', 'books\_count',
       'isbn', 'isbn13', 'authors', 'original\_publication\_year',
       'original\_title', 'title', 'language\_code', 'average\_rating',
       'ratings\_count', 'work\_ratings\_count', 'work\_text\_reviews\_count',
       'ratings\_1', 'ratings\_2', 'ratings\_3', 'ratings\_4', 'ratings\_5',
       'image\_url', 'small\_image\_url'],
      dtype='object')
\end{sphinxVerbatim}
}

Here’s a brief description for the features in the dataset.
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{books\_count}} represents the number of editions for a given work.

\item {} 
\sphinxcode{\sphinxupquote{gr\_best\_book\_id}} contains the most popular edition for a given work.

\item {} 
Columns \sphinxcode{\sphinxupquote{book\_id}}, \sphinxcode{\sphinxupquote{gr\_book\_id}}, \sphinxcode{\sphinxupquote{gr\_best\_book\_id}}, \sphinxcode{\sphinxupquote{work\_id}}, \sphinxcode{\sphinxupquote{isbn}} and \sphinxcode{\sphinxupquote{isbn13}} are different identifiers for the book. As we will see later, the book overviews are not included in this dataset and have been obtained by means of scraping. Each overview is identified with the \sphinxcode{\sphinxupquote{gr\_book\_id}} identifier, thus it is the link between both sources of information. Let us first check that it is a valid identifier (i.e., there are no null values and all identifiers are unique).

\end{itemize}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[11]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Unique values in }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{gr\PYGZus{}book\PYGZus{}id}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ column: }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{gr\PYGZus{}book\PYGZus{}id}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Print null values in }\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{gr\PYGZus{}book\PYGZus{}id}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{ column }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{data}\PYG{o}{.}\PYG{n}{gr\PYGZus{}book\PYGZus{}id}\PYG{p}{[}\PYG{n}{data}\PYG{o}{.}\PYG{n}{gr\PYGZus{}book\PYGZus{}id}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Unique values in 'gr\_book\_id' column: 10000
Print null values in 'gr\_book\_id' column Series([], Name: gr\_book\_id, dtype: int64)
\end{sphinxVerbatim}
}
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{gr\_book\_id}} is, in fact, a valid identifier, thus it is the one that we will be using. Furthermore, we also now know that there are no duplicated instances in the dataset, since at least one of its features has no repeated values. The remaining identifier columns can be deleted, as they do not provide any more meaningful information for the tasks that we are to perform.

\item {} 
As the name suggests, \sphinxcode{\sphinxupquote{authors}} contains the names of the authors of the book.

\item {} 
\sphinxcode{\sphinxupquote{original\_publication\_year}} indicates the year in which the book was published. We will not be using this information.

\item {} 
\sphinxcode{\sphinxupquote{title}} is the english title of the book.

\item {} 
\sphinxcode{\sphinxupquote{original\_title}} is the title of the book in its original language. We are primarily concerned with english textual information, hence \sphinxcode{\sphinxupquote{title}} is a more suitable feature.

\item {} 
\sphinxcode{\sphinxupquote{language\_code}} indicates the textual code assigned to the language of the book. This feature is particularly useful because it will help us get rid of non\sphinxhyphen{}English books.

\item {} 
\sphinxcode{\sphinxupquote{average\_rating}} is a floating value indicating the average rating of a book, ranging from 1 to 5. This feature does not provide relevant information for semantic search, thus it will be discarded. That notwithstanding, it could be used as a criteria to filter the query results, prioritizing those that have better ratings.

\item {} 
\sphinxcode{\sphinxupquote{ratings\_count}} indicates the number of registered ratings for a book. Analogously, \sphinxcode{\sphinxupquote{work\_ratings\_count}} and \sphinxcode{\sphinxupquote{work\_text\_reviews\_count}} indicate the number of ratings and reviews a work has in the platform, respectively. None of this information is useful for our work.

\item {} 
\sphinxcode{\sphinxupquote{ratings\_1}}, \sphinxcode{\sphinxupquote{ratings\_2}}, \sphinxcode{\sphinxupquote{ratings\_3}}, \sphinxcode{\sphinxupquote{ratings\_4}} and \sphinxcode{\sphinxupquote{ratings\_5}} characteristics hold the counts for each rating value. Again, this feature does not provide any relevant information to perform semantic search.

\item {} 
\sphinxcode{\sphinxupquote{image\_url}} and \sphinxcode{\sphinxupquote{small\_image\_url}} contain links to pictures of the book cover. Since images cannot be displayed in CLIs, we will discard this information too.

\end{itemize}


\section{Remove useless features.}
\label{\detokenize{Data preprocessing:Remove-useless-features.}}
Let’s get rid of all not useful features.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[12]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{columns\PYGZus{}to\PYGZus{}drop} \PYG{o}{=} \PYG{n+nb}{set}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{book\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gr\PYGZus{}best\PYGZus{}book\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{work\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{books\PYGZus{}count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{isbn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{isbn13}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{original\PYGZus{}publication\PYGZus{}year}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{original\PYGZus{}title}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{average\PYGZus{}rating}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings\PYGZus{}count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{work\PYGZus{}ratings\PYGZus{}count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{work\PYGZus{}text\PYGZus{}reviews\PYGZus{}count}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings\PYGZus{}1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings\PYGZus{}2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings\PYGZus{}3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
       \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings\PYGZus{}4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ratings\PYGZus{}5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{image\PYGZus{}url}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{small\PYGZus{}image\PYGZus{}url}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[13]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{data} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns\PYGZus{}to\PYGZus{}drop}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{n}{seed}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[13]:\,\hspace{\fboxrule}\hspace{\fboxsep}}      gr\_book\_id                        authors  \textbackslash{}
9394       38703                   Louis Sachar
898        53835  Edith Wharton, Maureen Howard
2398       43893                 Terry Goodkind
5906       31244                Charles Dickens
2343      497199               Charles Bukowski

                                  title language\_code
9394            Small Steps (Holes, \#2)           eng
898                The Age of Innocence           eng
2398  Naked Empire (Sword of Truth, \#8)         en-GB
5906                  Our Mutual Friend           eng
2343                           Factotum           NaN
\end{sphinxVerbatim}
}


\section{Integrate book overviews into the dataset.}
\label{\detokenize{Data preprocessing:Integrate-book-overviews-into-the-dataset.}}
Now that all useless characteristics have been deleted, let’s append the overviews to the dataframe. The book overviews are stored in a directory, one \sphinxcode{\sphinxupquote{txt}} file for each overview. We will first generate a dataframe containing all txt files in the directory. The filename for each \sphinxcode{\sphinxupquote{txt}} file is the \sphinxcode{\sphinxupquote{gr\_book\_id}} identifier.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[14]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{book\PYGZus{}overviews} \PYG{o}{=} \PYG{n}{utils}\PYG{o}{.}\PYG{n}{generate\PYGZus{}dataframe\PYGZus{}from\PYGZus{}sparse\PYGZus{}txts}\PYG{p}{(}\PYG{n}{DIR\PYGZus{}OVERVIEW}\PYG{p}{)}
\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[15]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of overviews in the dataset: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{book\PYGZus{}overviews}\PYG{o}{.}\PYG{n}{overview}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{book\PYGZus{}overviews}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Number of overviews in the dataset: 9956
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[15]:\,\hspace{\fboxrule}\hspace{\fboxsep}}      gr\_book\_id                                           overview
0              1  When Harry Potter and the Half-Blood Prince op{\ldots}
1             10  Six years of magic, adventure, and mystery mak{\ldots}
2       10000191  À sa naissance, Lisbeth est enlevée à sa mère {\ldots}
3          10006  The discovery of a mysterious notebook turns a{\ldots}
4        1000751  When orphaned 11-year-old Pollyanna comes to l{\ldots}
{\ldots}          {\ldots}                                                {\ldots}
9951     9995135  At long last, New York Times bestselling autho{\ldots}
9952       99955  Paine's daring prose paved the way for the Dec{\ldots}
9953        9998  The Woman in the Dunes, by celebrated writer a{\ldots}
9954     9998705  FLASH! Illuminated by lightning, a lifeless hu{\ldots}
9955     9999107  Witty, moving, and brilliantly entertaining, T{\ldots}

[9956 rows x 2 columns]
\end{sphinxVerbatim}
}

As it can be seen, there are \(9956\) book overviews, which is less than the number of instances in the other dataframe. Consequently, at least \(44\) books will have no overview. There are several strategies to merge both dataframes. In this case, we will allow having books with no overview (\sphinxstyleemphasis{left join} operation).

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[16]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{merge}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{book\PYGZus{}overviews}\PYG{p}{,} \PYG{n}{left\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gr\PYGZus{}book\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{right\PYGZus{}on}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gr\PYGZus{}book\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{how}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{left}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{data}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[16]:\,\hspace{\fboxrule}\hspace{\fboxsep}}      gr\_book\_id                      authors  \textbackslash{}
0        2767052              Suzanne Collins
1              3  J.K. Rowling, Mary GrandPré
2          41865              Stephenie Meyer
3           2657                   Harper Lee
4           4671          F. Scott Fitzgerald
{\ldots}          {\ldots}                          {\ldots}
9995     7130616                Ilona Andrews
9996      208324               Robert A. Caro
9997       77431              Patrick O'Brian
9998     8565083              Peggy Orenstein
9999        8914                  John Keegan

                                                  title language\_code  \textbackslash{}
0               The Hunger Games (The Hunger Games, \#1)           eng
1     Harry Potter and the Sorcerer's Stone (Harry P{\ldots}           eng
2                               Twilight (Twilight, \#1)         en-US
3                                 To Kill a Mockingbird           eng
4                                      The Great Gatsby           eng
{\ldots}                                                 {\ldots}           {\ldots}
9995                          Bayou Moon (The Edge, \#2)           eng
9996  Means of Ascent (The Years of Lyndon Johnson, \#2)           eng
9997                              The Mauritius Command           eng
9998  Cinderella Ate My Daughter: Dispatches from th{\ldots}           eng
9999                                The First World War           NaN

                                               overview
0     Winning will make you famous. Losing means cer{\ldots}
1     Harry Potter's life is miserable. His parents {\ldots}
2     About three things I was absolutely positive.F{\ldots}
3     The unforgettable novel of a childhood in a sl{\ldots}
4     On its first publication in 1925, The Great Ga{\ldots}
{\ldots}                                                 {\ldots}
9995  The Edge lies between worlds, on the border be{\ldots}
9996  Robert A. Caro's life of Lyndon Johnson, which{\ldots}
9997  "O'Brian's Aubrey-Maturin volumes actually con{\ldots}
9998  The acclaimed author of the groundbreaking bes{\ldots}
9999  The First World War created the modern world. {\ldots}

[10000 rows x 5 columns]
\end{sphinxVerbatim}
}


\section{Remove instances with invalid language codes.}
\label{\detokenize{Data preprocessing:Remove-instances-with-invalid-language-codes.}}
Let us now check whether there are noisy data in any of the selected characteristics. Starting off with the language code, we need to make sure that all data fed into the models is in English, being as they have been trained to derive semantic representations for English texts. To that end, let’s see how many language codes are in the dataset.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[17]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{data}\PYG{o}{.}\PYG{n}{language\PYGZus{}code}\PYG{o}{.}\PYG{n}{unique}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[17]:\,\hspace{\fboxrule}\hspace{\fboxsep}}array(['eng', 'en-US', 'en-CA', nan, 'spa', 'en-GB', 'fre', 'nl', 'ara',
       'por', 'ger', 'nor', 'jpn', 'en', 'vie', 'ind', 'pol', 'tur',
       'dan', 'fil', 'ita', 'per', 'swe', 'rum', 'mul', 'rus'],
      dtype=object)
\end{sphinxVerbatim}
}

As it can be seen, there are plenty of different languages. However, is the title and the overview of the book written in the language indicated in \sphinxcode{\sphinxupquote{language\_code}}? Let’s test it on some of the books with \sphinxcode{\sphinxupquote{language\_code = spa}}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[18]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{o}{.}\PYG{n}{language\PYGZus{}code} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spa}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{n}{seed}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[18]:\,\hspace{\fboxrule}\hspace{\fboxsep}}      gr\_book\_id                                      authors  \textbackslash{}
9472       53809                                 Paulo Coelho
83          7677                             Michael Crichton
9890     1365225                          José Emilio Pacheco
3751      140302                              Agatha Christie
4508       63032                               Roberto Bolaño
9222       61794                                    Anonymous
3476       31343                                    Anne Rice
5125       53926                           Mario Vargas Llosa
1799       22590  Philip K. Dick, David Alabort, Manuel Espín
555        10603                                 Stephen King

                                        title language\_code  \textbackslash{}
9472                                   Maktub           spa
83          Jurassic Park (Jurassic Park, \#1)           spa
9890              Las batallas en el desierto           spa
3751   Poirot Investiga (Hércules Poirot, \#3)           spa
4508                                     2666           spa
9222          La vida del Lazarillo de Tormes           spa
3476  Pandora (New Tales of the Vampires, \#1)           spa
5125               Travesuras de la niña mala           spa
1799                                     Ubik           spa
555                                      Cujo           spa

                                               overview
9472  Maktub não é um livro de conselhos, mas uma tr{\ldots}
83                                                  NaN
9890  Historia de un amor imposible, narración de un{\ldots}
3751                                                NaN
4508  A cuatro profesores de literatura, Pelletier, {\ldots}
9222  Lázaro es un muchacho desarrapado a quien la m{\ldots}
3476  Anne Rice, creator of the Vampire Lestat, the {\ldots}
5125  ¿Cuál es el verdadero rostro del amor?Ricardo {\ldots}
1799  Ubik is a 1969 science fict{\ldots}
555   Outside a peaceful town in central Maine, a mo{\ldots}
\end{sphinxVerbatim}
}

Since the title and the overview seems to be written in the language indicated in \sphinxcode{\sphinxupquote{language\_code}}, we will only choose those language codes mapped to English texts: \sphinxcode{\sphinxupquote{eng}}, \sphinxcode{\sphinxupquote{en\sphinxhyphen{}US}}, \sphinxcode{\sphinxupquote{en\sphinxhyphen{}CA}}, \sphinxcode{\sphinxupquote{en\sphinxhyphen{}GB}} and \sphinxcode{\sphinxupquote{en}}. It is, however, still necessary to check the instances in which the value for the language code is \sphinxcode{\sphinxupquote{NaN}}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[19]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{title}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{overview}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{[}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{language\PYGZus{}code}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{n}{seed}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[19]:\,\hspace{\fboxrule}\hspace{\fboxsep}}                                                  title  \textbackslash{}
3241  Born Free: A Lioness of Two Worlds (Story of E{\ldots}
3050                                         Stone Soup
4807  The Glass Magician (The Paper Magician Trilogy{\ldots}
9918                      Nothing's Fair in Fifth Grade
3971  Experiencing God: Knowing and Doing the Will o{\ldots}
9772  The Voyages of Doctor Dolittle (Doctor Dolittl{\ldots}
8179                                         First Love
2048                       Ramona the Pest (Ramona, \#2)
9559                    Relentless (The Lost Fleet, \#5)
9240    Truth Will Prevail (The Work and the Glory, \#3)

                                               overview
3241  There have been many accounts of the return to{\ldots}
3050  First published in 1947, this classic picture {\ldots}
4807  Three months after returning Magician Emery Th{\ldots}
9918  Jenny knows one thing for sure - Elsie Edwards{\ldots}
3971  Most Bible studies help people; this one chang{\ldots}
9772  The delightfully eccentric Doctor Dolittle, re{\ldots}
8179  An extraordinary portrait of true love that wi{\ldots}
2048  This is the second title in the hugely popular{\ldots}
9559  After successfully freeing Alliance POWs, "Bla{\ldots}
9240                                                NaN
\end{sphinxVerbatim}
}

More than \(10\%\) of the data has no language code. We verified that all of them are in English, thus they do not have to be deleted. Furthermore, the \sphinxcode{\sphinxupquote{language\_code}} feature is no longer needed.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[20]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{eng\PYGZus{}lc} \PYG{o}{=} \PYG{n+nb}{set}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{en}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{en\PYGZhy{}CA}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{en\PYGZhy{}US}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{en\PYGZhy{}GB}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{eng}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{language\PYGZus{}code}\PYG{o}{.}\PYG{n}{isin}\PYG{p}{(}\PYG{n}{eng\PYGZus{}lc}\PYG{p}{)}\PYG{p}{)} \PYG{o}{|} \PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{language\PYGZus{}code}\PYG{o}{.}\PYG{n}{isna}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{language\PYGZus{}code}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{n}{seed}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[20]:\,\hspace{\fboxrule}\hspace{\fboxsep}}      gr\_book\_id                                  authors  \textbackslash{}
7203      342994  Hans Christian Andersen, Rachel Isadora
8399       53200                          Stephen Hawking
8179    17899392           James Patterson, Emily Raymond
7047     2033217                             Daniel Silva
1091    17288661                             John Grisham
2050       13872                           Katherine Dunn
8558     1015311                             Ken Akamatsu
6090    21849362                                J.R. Ward
6774        7389         Brian K. Vaughan, Adrian Alphona
5760      522525             Carol Tavris, Elliot Aronson

                                                  title  \textbackslash{}
7203                              The Little Match Girl
8399                     Black Holes and Baby Universes
8179                                         First Love
7047                   Moscow Rules (Gabriel Allon, \#8)
1091                                       Sycamore Row
2050                                          Geek Love
8558                                 Love Hina, Vol. 01
6090        The Shadows (Black Dagger Brotherhood, \#13)
6774     Runaways, Vol. 1: Pride and Joy (Runaways, \#1)
5760  Mistakes Were Made (But Not by Me): Why We Jus{\ldots}

                                               overview
7203  The wares of the poor little match girl illumi{\ldots}
8399  NY Times bestseller. 13 extraordinary essays s{\ldots}
8179  An extraordinary portrait of true love that wi{\ldots}
7047  Now the death of a journalist leads Allon to R{\ldots}
1091  Seth Hubbard is a wealthy man dying of lung ca{\ldots}
2050  Geek Love is the story of the Binewskis, a car{\ldots}
8558  At the age of 5, Keitaro and his childhood swe{\ldots}
6090  Trez “Latimer” doesn’t really exist. And not j{\ldots}
6774  Meet Alex, Karolina, Gert, Chase, Molly and Ni{\ldots}
5760  Why do people dodge responsibility when things{\ldots}
\end{sphinxVerbatim}
}


\section{Remove noisy data from book titles.}
\label{\detokenize{Data preprocessing:Remove-noisy-data-from-book-titles.}}
The nomenclature utilized for the book titles is as follows: \(book\_title + (book\_saga\_name \ \#Nº\_book\_saga)\). Both the book title and the book saga can be valuable information. However, the book saga number, along with the \# symbol may be removed. I have defined a method called \sphinxcode{\sphinxupquote{clean\_book\_title}} that allows removing either all saga information or just the saga number.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[21]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{data}\PYG{o}{.}\PYG{n}{title} \PYG{o}{=} \PYG{p}{[}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{clean\PYGZus{}book\PYGZus{}title}\PYG{p}{(}\PYG{n}{title}\PYG{p}{)} \PYG{k}{for} \PYG{n}{title} \PYG{o+ow}{in} \PYG{n}{data}\PYG{o}{.}\PYG{n}{title}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{title}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{n}{seed}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxout}[21]:\,\hspace{\fboxrule}\hspace{\fboxsep}}7203                                The Little Match Girl
8399                       Black Holes and Baby Universes
8179                                           First Love
7047                         Moscow Rules (Gabriel Allon)
1091                                         Sycamore Row
2050                                            Geek Love
8558                                   Love Hina, Vol. 01
6090               The Shadows (Black Dagger Brotherhood)
6774           Runaways, Vol. 1: Pride and Joy (Runaways)
5760    Mistakes Were Made (But Not by Me): Why We Jus{\ldots}
Name: title, dtype: object
\end{sphinxVerbatim}
}


\section{Remove noisy data from book overviews.}
\label{\detokenize{Data preprocessing:Remove-noisy-data-from-book-overviews.}}
Luckily, text is automatically tokenized before being fed into any transformer model. That notwithstanding, there is still some work we need to do to clean our text beforehand, like removing special characters, removing extra blank spaces, etc. The \sphinxcode{\sphinxupquote{maketrans}} built\sphinxhyphen{}in method comes handy. It enables us to create a mapping table. We can create an empty mapping table, but the third argument of this function allows us to list all of the characters to remove during the translation process. On the
other hand, we will use the \sphinxcode{\sphinxupquote{re}} module to work with regular expressions with python to further fix some wrong text patterns.

For further details, please check the implementation included in the \sphinxcode{\sphinxupquote{utils}} module for the \sphinxcode{\sphinxupquote{clean\_overview}} method. Let’s see an example:

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[22]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{text} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{overview}\PYG{p}{[}\PYG{n}{data}\PYG{o}{.}\PYG{n}{gr\PYGZus{}book\PYGZus{}id} \PYG{o}{==} \PYG{l+m+mi}{5354}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BEFORE cleaning:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{ }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{text}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{AFTER cleaning:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{clean\PYGZus{}overview}\PYG{p}{(}\PYG{n}{text}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
}

{

\kern-\sphinxverbatimsmallskipamount\kern-\baselineskip
\kern+\FrameHeightAdjust\kern-\fboxrule
\vspace{\nbsphinxcodecellspacing}

\sphinxsetup{VerbatimColor={named}{white}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
BEFORE cleaning:
 Trumble is a minimum-security federal prison, a "camp," home to the usual assortment of relatively harmless criminals--drug dealers, bank robbers, swindlers, embezzlers, tax evaders, two Wall Street crooks, one doctor, at least five lawyers.And three former judges who call themselves the Brethren: one from Texas, one from California, and one from Mississippi. They meet each day in the law library, their turf at Trumble, where they write briefs, handle cases for other inmates, practice law without a license, and sometimes dispense jailhouse justice. And they spend hours writing letters. They are fine-tuning a mail scam, and it's starting to really work. The money is pouring in.Then their little scam goes awry. It ensnares the wrong victim, a powerful man on the outside, a man with dangerous friends, and the Brethren's days of quietly marking time are over.

AFTER cleaning:
Trumble is a minimum-security federal prison, a camp, home to the usual assortment of relatively harmless criminals drug dealers, bank robbers, swindlers, embezzlers, tax evaders, two Wall Street crooks, one doctor, at least five lawyers. And three former judges who call themselves the Brethren: one from Texas, one from California, and one from Mississippi. They meet each day in the law library, their turf at Trumble, where they write briefs, handle cases for other inmates, practice law without a license, and sometimes dispense jailhouse justice. And they spend hours writing letters. They are fine-tuning a mail scam, and it's starting to really work. The money is pouring in. Then their little scam goes awry. It ensnares the wrong victim, a powerful man on the outside, a man with dangerous friends, and the Brethren's days of quietly marking time are over.
\end{sphinxVerbatim}
}

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[23]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{data}\PYG{o}{.}\PYG{n}{overview} \PYG{o}{=} \PYG{p}{[}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{clean\PYGZus{}overview}\PYG{p}{(}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{overview}\PYG{p}{)}\PYG{p}{)} \PYG{k}{for} \PYG{n}{overview} \PYG{o+ow}{in} \PYG{n}{data}\PYG{o}{.}\PYG{n}{overview}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}
}

Once the preprocessing is done, the dataframe can be exported to a CSV file to avoid repeating these steps everytime we need to work with the cleaned data.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[24]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{n}{data}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gr\PYGZus{}book\PYGZus{}id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{to\PYGZus{}csv}\PYG{p}{(}\PYG{n}{DIR\PYGZus{}DATASET} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{books\PYGZus{}processed.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sep}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
}


\section{Annex. Code to perform data scraping.}
\label{\detokenize{Data preprocessing:Annex.-Code-to-perform-data-scraping.}}
The webpage for each book follow the format \sphinxcode{\sphinxupquote{https://www.goodreads.com/book/show/book\_id}}. For instance, \sphinxurl{https://www.goodreads.com/book/show/320} is the page containing information for the book “One Hundred Years of Solitude” by Gabriel García Márquez.

The overview is contained in an object called \sphinxcode{\sphinxupquote{readable stacked}} that can be seen inspecting the code of the page.

{
\sphinxsetup{VerbatimColor={named}{nbsphinx-code-bg}}
\sphinxsetup{VerbatimBorderColor={named}{nbsphinx-code-border}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\llap{\color{nbsphinxin}[ ]:\,\hspace{\fboxrule}\hspace{\fboxsep}}\PYG{k+kn}{import} \PYG{n+nn}{requests}
\PYG{k+kn}{from} \PYG{n+nn}{bs4} \PYG{k+kn}{import} \PYG{n}{BeautifulSoup}

\PYG{k}{def} \PYG{n+nf}{scrap\PYGZus{}book\PYGZus{}overview}\PYG{p}{(}\PYG{n}{book\PYGZus{}id}\PYG{p}{,} \PYG{n}{save}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{try}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} Connect to the page}
        \PYG{n}{url} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{https://www.goodreads.com/book/show/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{book\PYGZus{}id}\PYG{p}{)}
        \PYG{n}{response} \PYG{o}{=} \PYG{n}{requests}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{url}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} Instantiate a BeautifulSoup object.}
        \PYG{n}{soup} \PYG{o}{=} \PYG{n}{BeautifulSoup}\PYG{p}{(}\PYG{n}{response}\PYG{o}{.}\PYG{n}{text}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lxml}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} Access to the component}
        \PYG{n}{sec} \PYG{o}{=} \PYG{n}{soup}\PYG{o}{.}\PYG{n}{find}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{div}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{class}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{readable stacked}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} Extract the overview}
        \PYG{n}{overview} \PYG{o}{=} \PYG{n}{sec}\PYG{o}{.}\PYG{n}{findAll}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{span}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
        \PYG{c+c1}{\PYGZsh{} Store it, should you require it}
        \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{overview}\PYG{o}{.}\PYG{n}{text} \PYG{o+ow}{is} \PYG{k+kc}{None} \PYG{o+ow}{and} \PYG{n}{save}\PYG{p}{:}
            \PYG{n}{file} \PYG{o}{=} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{overviews/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{book\PYGZus{}id}\PYG{p}{)}\PYG{o}{+}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{.txt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{w}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
            \PYG{n}{file}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(}\PYG{n}{overview}\PYG{o}{.}\PYG{n}{text}\PYG{p}{)}
            \PYG{n}{file}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{overview}\PYG{o}{.}\PYG{n}{text}
    \PYG{k}{except}\PYG{p}{:}
        \PYG{k}{return} \PYG{k+kc}{None}
\end{sphinxVerbatim}
}




\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{c}
\item\relax\sphinxstyleindexentry{code\_utils.plotter}\sphinxstyleindexpageref{code:\detokenize{module-code_utils.plotter}}
\item\relax\sphinxstyleindexentry{code\_utils.utils}\sphinxstyleindexpageref{code:\detokenize{module-code_utils.utils}}
\indexspace
\bigletter{e}
\item\relax\sphinxstyleindexentry{experiment\_annoy\_ntrees}\sphinxstyleindexpageref{code:\detokenize{module-experiment_annoy_ntrees}}
\item\relax\sphinxstyleindexentry{experiment\_text\_summarization}\sphinxstyleindexpageref{code:\detokenize{module-experiment_text_summarization}}
\indexspace
\bigletter{l}
\item\relax\sphinxstyleindexentry{lexical\_search}\sphinxstyleindexpageref{code:\detokenize{module-lexical_search}}
\indexspace
\bigletter{s}
\item\relax\sphinxstyleindexentry{semantic\_search}\sphinxstyleindexpageref{code:\detokenize{module-semantic_search}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}